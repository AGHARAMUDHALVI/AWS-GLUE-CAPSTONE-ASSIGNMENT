{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::319474470582:role/service-role/AWSGlueServiceRole-Aghar\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: ed3903fe-e4d4-4f9a-b31d-0e491f499d57\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\nWaiting for session ed3903fe-e4d4-4f9a-b31d-0e491f499d57 to get into ready status...\nSession ed3903fe-e4d4-4f9a-b31d-0e491f499d57 has been created.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df=glueContext.create_dynamic_frame.from_catalog(\n                 database='bank_database-aghar',\n                 table_name='cleaned_bankdata').toDF()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 2,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import OneHotEncoder",
			"metadata": {
				"trusted": true
			},
			"execution_count": 3,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "catCols = ['job', 'marital', 'education', 'default','housing', 'loan', 'contact', 'poutcome']",
			"metadata": {
				"trusted": true
			},
			"execution_count": 4,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# The index of string vlaues multiple columns\nindexers = [\n    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n    for c in catCols\n]",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# The encode of indexed vlaues multiple columns\nencoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n    for indexer in indexers\n]\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 6,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Vectorizing encoded values\n#VectorAssembler to aseemble all the OneHotEncoded columns and the following numerical columns in one column. Call this new assembled column as: 'rawFeatures' :\nassembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"rawFeatures\")\n\nnumericCols = ['age', 'balance', 'duration',  'campaign', 'pdays', 'previous']",
			"metadata": {
				"trusted": true
			},
			"execution_count": 7,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#pipeline to transform each one of the categorical columns to as many Onehotencoded columns by first using StringIndexer and then OneHotEncoder.\nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages=indexers + encoders+ [assembler])\nmodel=pipeline.fit(df)\ntransformed = model.transform(df)\ntransformed.show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+--------+-------+-----------+---------------+-----------------+---------------+---------------+------------+---------------+----------------+-------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------+-----------------------+------------------------+--------------------+\n|age|        job|marital|education|default|balance|housing|loan| contact|duration|campaign|pdays|previous|poutcome|deposit|job_indexed|marital_indexed|education_indexed|default_indexed|housing_indexed|loan_indexed|contact_indexed|poutcome_indexed|job_indexed_encoded|marital_indexed_encoded|education_indexed_encoded|default_indexed_encoded|housing_indexed_encoded|loan_indexed_encoded|contact_indexed_encoded|poutcome_indexed_encoded|         rawFeatures|\n+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+--------+-------+-----------+---------------+-----------------+---------------+---------------+------------+---------------+----------------+-------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------+-----------------------+------------------------+--------------------+\n| 31| technician|married| tertiary|     no|   3382|     no|  no|cellular|     739|       1|   -1|       0| unknown|    yes|        2.0|            0.0|              1.0|            0.0|            0.0|         0.0|            0.0|             0.0|     (12,[2],[1.0])|          (3,[0],[1.0])|            (4,[1],[1.0])|          (2,[0],[1.0])|          (2,[0],[1.0])|       (2,[0],[1.0])|          (3,[0],[1.0])|           (4,[0],[1.0])|(32,[2,12,16,19,2...|\n| 54| management|married|secondary|     no|   1134|    yes|  no|cellular|    1330|       3|   -1|       0| unknown|    yes|        0.0|            0.0|              0.0|            0.0|            1.0|         0.0|            0.0|             0.0|     (12,[0],[1.0])|          (3,[0],[1.0])|            (4,[0],[1.0])|          (2,[0],[1.0])|          (2,[1],[1.0])|       (2,[0],[1.0])|          (3,[0],[1.0])|           (4,[0],[1.0])|(32,[0,12,15,19,2...|\n| 37|blue-collar|married|  primary|     no|    106|    yes|  no|cellular|     511|       1|  392|      13| failure|    yes|        1.0|            0.0|              2.0|            0.0|            1.0|         0.0|            0.0|             1.0|     (12,[1],[1.0])|          (3,[0],[1.0])|            (4,[2],[1.0])|          (2,[0],[1.0])|          (2,[1],[1.0])|       (2,[0],[1.0])|          (3,[0],[1.0])|           (4,[1],[1.0])|(32,[1,12,17,19,2...|\n| 29| technician| single|secondary|     no|   3466|     no|  no|cellular|     274|       1|   -1|       0| unknown|    yes|        2.0|            1.0|              0.0|            0.0|            0.0|         0.0|            0.0|             0.0|     (12,[2],[1.0])|          (3,[1],[1.0])|            (4,[0],[1.0])|          (2,[0],[1.0])|          (2,[0],[1.0])|       (2,[0],[1.0])|          (3,[0],[1.0])|           (4,[0],[1.0])|(32,[2,13,15,19,2...|\n| 29| technician| single| tertiary|     no|    615|    yes|  no|cellular|     373|       1|   -1|       0| unknown|    yes|        2.0|            1.0|              1.0|            0.0|            1.0|         0.0|            0.0|             0.0|     (12,[2],[1.0])|          (3,[1],[1.0])|            (4,[1],[1.0])|          (2,[0],[1.0])|          (2,[1],[1.0])|       (2,[0],[1.0])|          (3,[0],[1.0])|           (4,[0],[1.0])|(32,[2,13,16,19,2...|\n+---+-----------+-------+---------+-------+-------+-------+----+--------+--------+--------+-----+--------+--------+-------+-----------+---------------+-----------------+---------------+---------------+------------+---------------+----------------+-------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------+-----------------------+------------------------+--------------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "transformed.select('rawFeatures').printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- rawFeatures: vector (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "transformed.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- age: long (nullable = true)\n |-- job: string (nullable = true)\n |-- marital: string (nullable = true)\n |-- education: string (nullable = true)\n |-- default: string (nullable = true)\n |-- balance: long (nullable = true)\n |-- housing: string (nullable = true)\n |-- loan: string (nullable = true)\n |-- contact: string (nullable = true)\n |-- duration: long (nullable = true)\n |-- campaign: long (nullable = true)\n |-- pdays: long (nullable = true)\n |-- previous: long (nullable = true)\n |-- poutcome: string (nullable = true)\n |-- deposit: string (nullable = true)\n |-- job_indexed: double (nullable = false)\n |-- marital_indexed: double (nullable = false)\n |-- education_indexed: double (nullable = false)\n |-- default_indexed: double (nullable = false)\n |-- housing_indexed: double (nullable = false)\n |-- loan_indexed: double (nullable = false)\n |-- contact_indexed: double (nullable = false)\n |-- poutcome_indexed: double (nullable = false)\n |-- job_indexed_encoded: vector (nullable = true)\n |-- marital_indexed_encoded: vector (nullable = true)\n |-- education_indexed_encoded: vector (nullable = true)\n |-- default_indexed_encoded: vector (nullable = true)\n |-- housing_indexed_encoded: vector (nullable = true)\n |-- loan_indexed_encoded: vector (nullable = true)\n |-- contact_indexed_encoded: vector (nullable = true)\n |-- poutcome_indexed_encoded: vector (nullable = true)\n |-- rawFeatures: vector (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "scaler = StandardScaler(inputCol='rawFeatures', outputCol='scaled_rawFeatures')\nscaler_model = scaler.fit(transformed)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "(trainingData, testData) = transformed.randomSplit([0.7, 0.3],seed = 11)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 12,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "trainingData.show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-------+-------+---------+-------+-------+-------+----+---------+--------+--------+-----+--------+--------+-------+-----------+---------------+-----------------+---------------+---------------+------------+---------------+----------------+-------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------+-----------------------+------------------------+--------------------+\n|age|    job|marital|education|default|balance|housing|loan|  contact|duration|campaign|pdays|previous|poutcome|deposit|job_indexed|marital_indexed|education_indexed|default_indexed|housing_indexed|loan_indexed|contact_indexed|poutcome_indexed|job_indexed_encoded|marital_indexed_encoded|education_indexed_encoded|default_indexed_encoded|housing_indexed_encoded|loan_indexed_encoded|contact_indexed_encoded|poutcome_indexed_encoded|         rawFeatures|\n+---+-------+-------+---------+-------+-------+-------+----+---------+--------+--------+-----+--------+--------+-------+-----------+---------------+-----------------+---------------+---------------+------------+---------------+----------------+-------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------+-----------------------+------------------------+--------------------+\n| 19|student| single|secondary|     no|    372|     no|  no|telephone|     444|       3|   -1|       0| unknown|    yes|        7.0|            1.0|              0.0|            0.0|            0.0|         0.0|            2.0|             0.0|     (12,[7],[1.0])|          (3,[1],[1.0])|            (4,[0],[1.0])|          (2,[0],[1.0])|          (2,[0],[1.0])|       (2,[0],[1.0])|          (3,[2],[1.0])|           (4,[0],[1.0])|(32,[7,13,15,19,2...|\n| 20|student| single|  unknown|     no|    801|     no|  no| cellular|     244|       2|   -1|       0| unknown|    yes|        7.0|            1.0|              3.0|            0.0|            0.0|         0.0|            0.0|             0.0|     (12,[7],[1.0])|          (3,[1],[1.0])|            (4,[3],[1.0])|          (2,[0],[1.0])|          (2,[0],[1.0])|       (2,[0],[1.0])|          (3,[0],[1.0])|           (4,[0],[1.0])|(32,[7,13,18,19,2...|\n| 23| admin.| single|secondary|     no|   1104|    yes|  no|  unknown|     120|       2|   -1|       0| unknown|     no|        3.0|            1.0|              0.0|            0.0|            1.0|         0.0|            1.0|             0.0|     (12,[3],[1.0])|          (3,[1],[1.0])|            (4,[0],[1.0])|          (2,[0],[1.0])|          (2,[1],[1.0])|       (2,[0],[1.0])|          (3,[1],[1.0])|           (4,[0],[1.0])|(32,[3,13,15,19,2...|\n| 24|student| single|  primary|     no|   1235|     no|  no| cellular|     334|       4|   93|       6|   other|    yes|        7.0|            1.0|              2.0|            0.0|            0.0|         0.0|            0.0|             3.0|     (12,[7],[1.0])|          (3,[1],[1.0])|            (4,[2],[1.0])|          (2,[0],[1.0])|          (2,[0],[1.0])|       (2,[0],[1.0])|          (3,[0],[1.0])|           (4,[3],[1.0])|(32,[7,13,17,19,2...|\n| 24|student| single|  unknown|     no|    358|     no|  no| cellular|     245|       1|  384|       1|   other|    yes|        7.0|            1.0|              3.0|            0.0|            0.0|         0.0|            0.0|             3.0|     (12,[7],[1.0])|          (3,[1],[1.0])|            (4,[3],[1.0])|          (2,[0],[1.0])|          (2,[0],[1.0])|       (2,[0],[1.0])|          (3,[0],[1.0])|           (4,[3],[1.0])|(32,[7,13,18,19,2...|\n+---+-------+-------+---------+-------+-------+-------+----+---------+--------+--------+-----+--------+--------+-------+-----------+---------------+-----------------+---------------+---------------+------------+---------------+----------------+-------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------+-----------------------+------------------------+--------------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "testData.show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+-----------+-------+---------+-------+-------+-------+----+---------+--------+--------+-----+--------+--------+-------+-----------+---------------+-----------------+---------------+---------------+------------+---------------+----------------+-------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------+-----------------------+------------------------+--------------------+\n|age|        job|marital|education|default|balance|housing|loan|  contact|duration|campaign|pdays|previous|poutcome|deposit|job_indexed|marital_indexed|education_indexed|default_indexed|housing_indexed|loan_indexed|contact_indexed|poutcome_indexed|job_indexed_encoded|marital_indexed_encoded|education_indexed_encoded|default_indexed_encoded|housing_indexed_encoded|loan_indexed_encoded|contact_indexed_encoded|poutcome_indexed_encoded|         rawFeatures|\n+---+-----------+-------+---------+-------+-------+-------+----+---------+--------+--------+-----+--------+--------+-------+-----------+---------------+-----------------+---------------+---------------+------------+---------------+----------------+-------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------+-----------------------+------------------------+--------------------+\n| 22|    student| single|secondary|     no|    948|     no|  no|telephone|     215|       1|  197|       2| failure|    yes|        7.0|            1.0|              0.0|            0.0|            0.0|         0.0|            2.0|             1.0|     (12,[7],[1.0])|          (3,[1],[1.0])|            (4,[0],[1.0])|          (2,[0],[1.0])|          (2,[0],[1.0])|       (2,[0],[1.0])|          (3,[2],[1.0])|           (4,[1],[1.0])|(32,[7,13,15,19,2...|\n| 22|    student| single|secondary|     no|   2162|     no|  no|telephone|     168|       1|   -1|       0| unknown|     no|        7.0|            1.0|              0.0|            0.0|            0.0|         0.0|            2.0|             0.0|     (12,[7],[1.0])|          (3,[1],[1.0])|            (4,[0],[1.0])|          (2,[0],[1.0])|          (2,[0],[1.0])|       (2,[0],[1.0])|          (3,[2],[1.0])|           (4,[0],[1.0])|(32,[7,13,15,19,2...|\n| 25|blue-collar|married|secondary|     no|   1782|    yes|  no|  unknown|     549|       1|   -1|       0| unknown|     no|        1.0|            0.0|              0.0|            0.0|            1.0|         0.0|            1.0|             0.0|     (12,[1],[1.0])|          (3,[0],[1.0])|            (4,[0],[1.0])|          (2,[0],[1.0])|          (2,[1],[1.0])|       (2,[0],[1.0])|          (3,[1],[1.0])|           (4,[0],[1.0])|(32,[1,12,15,19,2...|\n| 25|blue-collar| single|  primary|     no|    292|    yes| yes| cellular|    1027|       2|   -1|       0| unknown|    yes|        1.0|            1.0|              2.0|            0.0|            1.0|         1.0|            0.0|             0.0|     (12,[1],[1.0])|          (3,[1],[1.0])|            (4,[2],[1.0])|          (2,[0],[1.0])|          (2,[1],[1.0])|       (2,[1],[1.0])|          (3,[0],[1.0])|           (4,[0],[1.0])|(32,[1,13,17,19,2...|\n| 25|blue-collar| single|secondary|     no|   1857|    yes|  no| cellular|     195|       2|   91|      10| success|    yes|        1.0|            1.0|              0.0|            0.0|            1.0|         0.0|            0.0|             2.0|     (12,[1],[1.0])|          (3,[1],[1.0])|            (4,[0],[1.0])|          (2,[0],[1.0])|          (2,[1],[1.0])|       (2,[0],[1.0])|          (3,[0],[1.0])|           (4,[2],[1.0])|(32,[1,13,15,19,2...|\n+---+-----------+-------+---------+-------+-------+-------+----+---------+--------+--------+-----+--------+--------+-------+-----------+---------------+-----------------+---------------+---------------+------------+---------------+----------------+-------------------+-----------------------+-------------------------+-----------------------+-----------------------+--------------------+-----------------------+------------------------+--------------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "trainingData.columns",
			"metadata": {
				"trusted": true
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit', 'job_indexed', 'marital_indexed', 'education_indexed', 'default_indexed', 'housing_indexed', 'loan_indexed', 'contact_indexed', 'poutcome_indexed', 'job_indexed_encoded', 'marital_indexed_encoded', 'education_indexed_encoded', 'default_indexed_encoded', 'housing_indexed_encoded', 'loan_indexed_encoded', 'contact_indexed_encoded', 'poutcome_indexed_encoded', 'rawFeatures']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "trainingData.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "7821\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "testData.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "3341\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "trainingData.columns",
			"metadata": {
				"trusted": true
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit', 'job_indexed', 'marital_indexed', 'education_indexed', 'default_indexed', 'housing_indexed', 'loan_indexed', 'contact_indexed', 'poutcome_indexed', 'job_indexed_encoded', 'marital_indexed_encoded', 'education_indexed_encoded', 'default_indexed_encoded', 'housing_indexed_encoded', 'loan_indexed_encoded', 'contact_indexed_encoded', 'poutcome_indexed_encoded', 'rawFeatures']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "trainingData.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- age: long (nullable = true)\n |-- job: string (nullable = true)\n |-- marital: string (nullable = true)\n |-- education: string (nullable = true)\n |-- default: string (nullable = true)\n |-- balance: long (nullable = true)\n |-- housing: string (nullable = true)\n |-- loan: string (nullable = true)\n |-- contact: string (nullable = true)\n |-- duration: long (nullable = true)\n |-- campaign: long (nullable = true)\n |-- pdays: long (nullable = true)\n |-- previous: long (nullable = true)\n |-- poutcome: string (nullable = true)\n |-- deposit: string (nullable = true)\n |-- job_indexed: double (nullable = false)\n |-- marital_indexed: double (nullable = false)\n |-- education_indexed: double (nullable = false)\n |-- default_indexed: double (nullable = false)\n |-- housing_indexed: double (nullable = false)\n |-- loan_indexed: double (nullable = false)\n |-- contact_indexed: double (nullable = false)\n |-- poutcome_indexed: double (nullable = false)\n |-- job_indexed_encoded: vector (nullable = true)\n |-- marital_indexed_encoded: vector (nullable = true)\n |-- education_indexed_encoded: vector (nullable = true)\n |-- default_indexed_encoded: vector (nullable = true)\n |-- housing_indexed_encoded: vector (nullable = true)\n |-- loan_indexed_encoded: vector (nullable = true)\n |-- contact_indexed_encoded: vector (nullable = true)\n |-- poutcome_indexed_encoded: vector (nullable = true)\n |-- rawFeatures: vector (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "trainingData.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "7821\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "testData.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "3341\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "trainingData.repartition(1).write.option(\"header\", \"true\").option(\"schema\",\"true\").mode('overwrite').parquet('s3://aghar-awsglue-capstone/Bank_Data/processed_BankData/train-data/')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 170,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "testData.repartition(1).write.option(\"header\", \"true\").option(\"schema\",\"true\").mode('overwrite').parquet('s3://aghar-awsglue-capstone/Bank_Data/processed_BankData/test-data/')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 171,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "# Modeling",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "Next step is to Modeling:\n\nList of few Classification Algorithms from Spark ML\n\nLogisticRegression\n\nDecisionTreeClassifier\n\nRandomForestClassifier\n\nGradient-boosted tree classifier\n\nNaiveBayes\n\nSupport Vector Machine",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "# LogisticRegression",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import QuantileDiscretizer",
			"metadata": {
				"trusted": true
			},
			"execution_count": 172,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"loan_indexed\", featuresCol=\"rawFeatures\")\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 173,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"loan_indexed\", featuresCol=\"rawFeatures\")\n#Training algo\nlrModel = lr.fit(trainingData)\nlr_prediction = lrModel.transform(testData)\nlr_prediction.select(\"prediction\", \"loan_indexed\", \"rawFeatures\").show()\nevaluator = MulticlassClassificationEvaluator(labelCol=\"loan_indexed\", predictionCol=\"prediction\", metricName=\"accuracy\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+--------------------+\n|prediction|loan_indexed|         rawFeatures|\n+----------+------------+--------------------+\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,13,17,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       0.0|         0.0|(32,[4,12,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       1.0|         1.0|(32,[3,13,15,19,2...|\n|       1.0|         1.0|(32,[1,13,15,19,2...|\n|       1.0|         1.0|(32,[0,12,16,20,2...|\n|       0.0|         0.0|(32,[2,13,15,19,2...|\n|       0.0|         0.0|(32,[2,13,18,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,12,15,19,2...|\n+----------+------------+--------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Evaluating accuracy of LogisticRegression.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.mllib.evaluation import BinaryClassificationMetrics\nout = lrModel.transform(testData)\\\n .select(\"prediction\",\"loan_indexed\")\\\n .rdd.map(lambda x: (float(x[0]), float(x[1])))\nlr_metrics = BinaryClassificationMetrics(out)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 94,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderPR\"+\" : \"+str(lr_metrics.areaUnderPR))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 130,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderPR : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderROC\"+\" : \"+str(lr_metrics.areaUnderROC))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 131,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderROC : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Use the MulticlassClassificationEvaluator to evaluate the model's accuracy\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"loan_indexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(lr_prediction)\nprint(\"Accuracy:\", accuracy)\n# Select the \"prediction\" and \"label\" columns\npredictions_df = lr_prediction.select([\"prediction\", \"loan_indexed\"])\n\n# Convert the predictions and labels to Pandas dataframes for easier inspection\npredictions_pd = predictions_df.toPandas()\n\n# Print the first 10 predictions and their corresponding true labels\nprint(predictions_pd.head(10))\n# Set the hyperparameters for the logistic regression model\nlr = LogisticRegression(labelCol='loan_indexed', featuresCol='rawFeatures')\n\n# Fit the model to the training data\nlr_model = lr.fit(trainingData)\n\n# Make predictions on the test data\npredictions = lr_model.transform(testData)\n# Save the model to a file\n#lr_model.save(\"logistic_regression_model1\")\n\n# Load the saved model\n#loaded_model = LogisticRegression.load(\"/content/logistic_regression_model1\")\n\naccuracy = evaluator.evaluate(predictions)\nprint(\"Accuracy:\", accuracy)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy: 1.0\n   prediction  loan_indexed\n0         0.0           0.0\n1         0.0           0.0\n2         0.0           0.0\n3         1.0           1.0\n4         0.0           0.0\n5         0.0           0.0\n6         0.0           0.0\n7         0.0           0.0\n8         0.0           0.0\n9         0.0           0.0\nAccuracy: 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "lr_accuracy = evaluator.evaluate(lr_prediction)\nprint(\"Accuracy:\", lr_accuracy)\nprint(\"Accuracy_LogisticRegression is = %g\"% (lr_accuracy))\nprint(\"Test Error_LogisticRegression = %g \" % (1.0 - lr_accuracy))\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy: 1.0\nAccuracy_LogisticRegression is = 1\nTest Error_LogisticRegression = 0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# DecisionTreeClassifier",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import DecisionTreeClassifier\ndt = DecisionTreeClassifier(labelCol=\"loan_indexed\", featuresCol=\"rawFeatures\")\ndt_model = dt.fit(trainingData)\ndt_prediction = dt_model.transform(testData)\ndt_prediction.select(\"prediction\", \"loan_indexed\", \"rawFeatures\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+--------------------+\n|prediction|loan_indexed|         rawFeatures|\n+----------+------------+--------------------+\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,13,17,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       0.0|         0.0|(32,[4,12,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       1.0|         1.0|(32,[3,13,15,19,2...|\n|       1.0|         1.0|(32,[1,13,15,19,2...|\n|       1.0|         1.0|(32,[0,12,16,20,2...|\n|       0.0|         0.0|(32,[2,13,15,19,2...|\n|       0.0|         0.0|(32,[2,13,18,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,12,15,19,2...|\n+----------+------------+--------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Evaluating accuracy of DecisionTreeClassifier.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "dt_accuracy = evaluator.evaluate(dt_prediction)\nprint(\"Accuracy of DecisionTreeClassifier is = %g\"% (dt_accuracy))\nprint(\"Test Error of DecisionTreeClassifier = %g \" % (1.0 - dt_accuracy))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy of DecisionTreeClassifier is = 1\nTest Error of DecisionTreeClassifier = 0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.mllib.evaluation import BinaryClassificationMetrics\nout = dt_model.transform(testData)\\\n .select(\"prediction\",\"loan_indexed\")\\\n .rdd.map(lambda x: (float(x[0]), float(x[1])))\ndt_metrics = BinaryClassificationMetrics(out)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 97,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderPR\"+\" : \"+str(dt_metrics.areaUnderPR))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 128,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderPR : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderROC\"+\" : \"+str(dt_metrics.areaUnderROC))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 129,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderROC : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# RandomForestClassifier",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import RandomForestClassifier\nrf = DecisionTreeClassifier(labelCol=\"loan_indexed\", featuresCol=\"rawFeatures\")\nrf_model = rf.fit(trainingData)\nrf_prediction = rf_model.transform(testData)\nrf_prediction.select(\"prediction\", \"loan_indexed\", \"rawFeatures\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+--------------------+\n|prediction|loan_indexed|         rawFeatures|\n+----------+------------+--------------------+\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,13,17,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       0.0|         0.0|(32,[4,12,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       1.0|         1.0|(32,[3,13,15,19,2...|\n|       1.0|         1.0|(32,[1,13,15,19,2...|\n|       1.0|         1.0|(32,[0,12,16,20,2...|\n|       0.0|         0.0|(32,[2,13,15,19,2...|\n|       0.0|         0.0|(32,[2,13,18,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,12,15,19,2...|\n+----------+------------+--------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Evaluating accuracy of RandomForestClassifier.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "accuracy = evaluator.evaluate(rf_prediction)\nprint(\"Accuracy:\", accuracy)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy: 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "rf_accuracy = evaluator.evaluate(rf_prediction)\nprint(\"Accuracy of RandomForestClassifier is = %g\"% (rf_accuracy))\nprint(\"Test Error of RandomForestClassifier  = %g \" % (1.0 - rf_accuracy))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy of RandomForestClassifier is = 1\nTest Error of RandomForestClassifier  = 0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.mllib.evaluation import BinaryClassificationMetrics\nout = rf_model.transform(testData)\\\n .select(\"prediction\",\"loan_indexed\")\\\n .rdd.map(lambda x: (float(x[0]), float(x[1])))\nrf_metrics = BinaryClassificationMetrics(out)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 100,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderPR\"+\" : \"+str(rf_metrics.areaUnderPR))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 126,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderPR : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderROC\"+\" : \"+str(rf_metrics.areaUnderROC))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 127,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderROC : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Gradient-boosted tree classifier",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(labelCol=\"loan_indexed\", featuresCol=\"rawFeatures\",maxIter=10)\ngbt_model = gbt.fit(trainingData)\ngbt_prediction = gbt_model.transform(testData)\ngbt_prediction.select(\"prediction\", \"loan_indexed\", \"rawFeatures\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 40,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+--------------------+\n|prediction|loan_indexed|         rawFeatures|\n+----------+------------+--------------------+\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,13,17,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       0.0|         0.0|(32,[4,12,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       1.0|         1.0|(32,[3,13,15,19,2...|\n|       1.0|         1.0|(32,[1,13,15,19,2...|\n|       1.0|         1.0|(32,[0,12,16,20,2...|\n|       0.0|         0.0|(32,[2,13,15,19,2...|\n|       0.0|         0.0|(32,[2,13,18,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,12,15,19,2...|\n+----------+------------+--------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Evaluate accuracy of Gradient-boosted.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "gbt_accuracy = evaluator.evaluate(gbt_prediction)\nprint(\"Accuracy of Gradient-boosted tree classifie is = %g\"% (gbt_accuracy))\nprint(\"Test Error of Gradient-boosted tree classifie %g\"% (1.0 - gbt_accuracy))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy of Gradient-boosted tree classifie is = 1\nTest Error of Gradient-boosted tree classifie 0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.mllib.evaluation import BinaryClassificationMetrics\nout = gbt_model.transform(testData)\\\n .select(\"prediction\",\"loan_indexed\")\\\n .rdd.map(lambda x: (float(x[0]), float(x[1])))\ngbt_metrics = BinaryClassificationMetrics(out)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 103,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderPR\"+\" : \"+str(gbt_metrics.areaUnderPR))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 124,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderPR : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderROC\"+\" : \"+str(gbt_metrics.areaUnderROC))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 125,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderROC : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# NaiveBayes",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import NaiveBayes\nnb = NaiveBayes(labelCol=\"loan_indexed\", featuresCol=\"rawFeatures\")\nnb_model = nb.fit(trainingData)\nnb_prediction = nb_model.transform(testData)\nnb_prediction.select(\"prediction\", \"loan_indexed\", \"rawFeatures\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+--------------------+\n|prediction|loan_indexed|         rawFeatures|\n+----------+------------+--------------------+\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,13,17,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       0.0|         0.0|(32,[4,12,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       1.0|         1.0|(32,[3,13,15,19,2...|\n|       1.0|         1.0|(32,[1,13,15,19,2...|\n|       1.0|         1.0|(32,[0,12,16,20,2...|\n|       0.0|         0.0|(32,[2,13,15,19,2...|\n|       0.0|         0.0|(32,[2,13,18,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,12,15,19,2...|\n+----------+------------+--------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Evaluating accuracy of NaiveBayes.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "nb_accuracy = evaluator.evaluate(nb_prediction)\nprint(\"Accuracy of NaiveBayes is  = %g\"% (nb_accuracy))\nprint(\"Test Error of NaiveBayes  = %g \" % (1.0 - nb_accuracy))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy of NaiveBayes is  = 1\nTest Error of NaiveBayes  = 0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.mllib.evaluation import BinaryClassificationMetrics\nout = nb_model.transform(testData)\\\n .select(\"prediction\",\"loan_indexed\")\\\n .rdd.map(lambda x: (float(x[0]), float(x[1])))\nnb_metrics = BinaryClassificationMetrics(out)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 106,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderPR\"+\" : \"+str(nb_metrics.areaUnderPR))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 122,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderPR : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderROC\"+\" : \"+str(nb_metrics.areaUnderROC))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 123,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderROC : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Support Vector Machine",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.classification import LinearSVC\nsvm = LinearSVC(labelCol=\"loan_indexed\", featuresCol=\"rawFeatures\")\nsvm_model = svm.fit(trainingData)\nsvm_prediction = svm_model.transform(testData)\nsvm_prediction.select(\"prediction\", \"loan_indexed\", \"rawFeatures\").show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 44,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+------------+--------------------+\n|prediction|loan_indexed|         rawFeatures|\n+----------+------------+--------------------+\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[7,13,15,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,13,17,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[1,13,15,19,2...|\n|       0.0|         0.0|(32,[0,13,16,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       0.0|         0.0|(32,[4,12,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       0.0|         0.0|(32,[4,13,15,19,2...|\n|       1.0|         1.0|(32,[3,13,15,19,2...|\n|       1.0|         1.0|(32,[1,13,15,19,2...|\n|       1.0|         1.0|(32,[0,12,16,20,2...|\n|       0.0|         0.0|(32,[2,13,15,19,2...|\n|       0.0|         0.0|(32,[2,13,18,19,2...|\n|       0.0|         0.0|(32,[1,12,15,19,2...|\n|       1.0|         1.0|(32,[1,12,15,19,2...|\n+----------+------------+--------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Evaluating the accuracy of Support Vector Mac",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "svm_accuracy = evaluator.evaluate(svm_prediction)\nprint(\"Accuracy of Support Vector Machine is = %g\"% (svm_accuracy))\nprint(\"Test Error of Support Vector Machine = %g \" % (1.0 - svm_accuracy))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 45,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy of Support Vector Machine is = 1\nTest Error of Support Vector Machine = 0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.mllib.evaluation import BinaryClassificationMetrics\nout = svm_model.transform(testData)\\\n .select(\"prediction\",\"loan_indexed\")\\\n .rdd.map(lambda x: (float(x[0]), float(x[1])))\nsvm_metrics = BinaryClassificationMetrics(out)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 109,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderPR\"+\" : \"+str(svm_metrics.areaUnderPR))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 120,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderPR : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderROC\"+\" : \"+str(svm_metrics.areaUnderROC))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 121,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderROC : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Hyperparameter tuning and CrossValidation",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Define the hyperparameters to tune\nhyperparameters = [\n    {'regParam': [0.1, 0.01, 0.001], 'elasticNetParam': [0.0, 0.5, 1.0]},\n    {'regParam': [0.1, 0.01, 0.001], 'elasticNetParam': [0.0, 0.5, 1.0], 'maxIter': [10, 50, 100]}\n]",
			"metadata": {
				"trusted": true
			},
			"execution_count": 46,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "param_grid = ParamGridBuilder().addGrid(lr.regParam, hyperparameters[0]['regParam'])\\\n                               .addGrid(lr.elasticNetParam, hyperparameters[0]['elasticNetParam'])\\\n                               .build()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 47,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "cv = CrossValidator(estimator=lr, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=2)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 48,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "model = cv.fit(trainingData)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 49,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "model.params",
			"metadata": {
				"trusted": true
			},
			"execution_count": 50,
			"outputs": [
				{
					"name": "stdout",
					"text": "[Param(parent='CrossValidatorModel_3c0bffd389f8', name='estimator', doc='estimator to be cross-validated'), Param(parent='CrossValidatorModel_3c0bffd389f8', name='estimatorParamMaps', doc='estimator param maps'), Param(parent='CrossValidatorModel_3c0bffd389f8', name='evaluator', doc='evaluator used to select hyper-parameters that maximize the validator metric'), Param(parent='CrossValidatorModel_3c0bffd389f8', name='seed', doc='random seed.')]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "model.bestModel",
			"metadata": {
				"trusted": true
			},
			"execution_count": 51,
			"outputs": [
				{
					"name": "stdout",
					"text": "LogisticRegressionModel: uid = LogisticRegression_8f5adf87b2e3, numClasses = 2, numFeatures = 32\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "predictions = model.transform(testData)\n\naccuracy = evaluator.evaluate(lr_prediction)\nprint(\"Accuracy: \", accuracy)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "Accuracy:  1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from pyspark.mllib.evaluation import BinaryClassificationMetrics\nout = model.transform(testData)\\\n .select(\"prediction\",\"loan_indexed\")\\\n .rdd.map(lambda x: (float(x[0]), float(x[1])))\ncv_metrics = BinaryClassificationMetrics(out)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 115,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderPR\"+\" : \"+str(cv_metrics.areaUnderPR))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 118,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderPR : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"areaUnderROC\"+\" : \"+str(cv_metrics.areaUnderROC))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 119,
			"outputs": [
				{
					"name": "stdout",
					"text": "areaUnderROC : 1.0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "metrics=[lr_metrics,dt_metrics,rf_metrics,gbt_metrics,nb_metrics,svm_metrics,cv_metrics]",
			"metadata": {
				"trusted": true
			},
			"execution_count": 151,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "models=[\"logistic regression\",\"decisiontree classification\",\"randomforest classification\",\"GBT classification\",\"NaiveBayes classification\",\"SVM classification\",\"CV_Logistic regression\"]",
			"metadata": {
				"trusted": true
			},
			"execution_count": 152,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.types import StructType,StructField,DoubleType,StringType\nmetrics_schema = StructType([ \\\n    StructField(\"Model\",StringType(),True), \\\n    StructField(\"areaUnderPR\",DoubleType(),True), \\\n    StructField(\"areaUnderROC\",DoubleType(),True), \\\n  ])",
			"metadata": {
				"trusted": true
			},
			"execution_count": 153,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "metrics_data=[]\nli=[]\nj=0\nfor i in metrics:\n    li.append(models[j])\n    j=j+1\n    li.append(i.areaUnderPR)\n    li.append(i.areaUnderROC)\n    metrics_data.append(li)\n    li=[]\nprint(metrics_data)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 163,
			"outputs": [
				{
					"name": "stdout",
					"text": "[['logistic regression', 1.0, 1.0], ['decisiontree classification', 1.0, 1.0], ['randomforest classification', 1.0, 1.0], ['GBT classification', 1.0, 1.0], ['NaiveBayes classification', 1.0, 1.0], ['SVM classification', 1.0, 1.0], ['CV_Logistic regression', 1.0, 1.0]]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "metrics_df=spark.createDataFrame(data=metrics_data,schema=metrics_schema)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 164,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "metrics_df.show(truncate=35)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 167,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------------------------+-----------+------------+\n|                      Model|areaUnderPR|areaUnderROC|\n+---------------------------+-----------+------------+\n|        logistic regression|        1.0|         1.0|\n|decisiontree classification|        1.0|         1.0|\n|randomforest classification|        1.0|         1.0|\n|         GBT classification|        1.0|         1.0|\n|  NaiveBayes classification|        1.0|         1.0|\n|         SVM classification|        1.0|         1.0|\n|     CV_Logistic regression|        1.0|         1.0|\n+---------------------------+-----------+------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "metrics_df.repartition(1).write.option(\"header\", \"true\").option(\"schema\",\"true\").mode('overwrite').parquet('s3://aghar-awsglue-capstone/Bank_Data/output_metrics/')",
			"metadata": {
				"trusted": true
			},
			"execution_count": 169,
			"outputs": []
		}
	]
}