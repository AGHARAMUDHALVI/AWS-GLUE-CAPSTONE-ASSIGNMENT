{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9adf80",
   "metadata": {},
   "source": [
    "# SPARK hands-on exercises and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05ab98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dfd2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/agharamudhalvi/spark-3.3.0-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f34daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c267cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8b196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09692417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 20:09:18 WARN Utils: Your hostname, TIGER02310 resolves to a loopback address: 127.0.1.1; using 172.18.176.1 instead (on interface eth1)\n",
      "23/01/30 20:09:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 20:09:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Assignment\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef8381",
   "metadata": {},
   "source": [
    "# DAY 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d8691",
   "metadata": {},
   "source": [
    "# WHAT IS BIGDATA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dacd54",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Apache Spark is an open-source, distributed computing framework designed for large-scale data processing. In the context of big data, Spark is used for big data processing and analysis by enabling in-memory computing, which provides faster processing speed compared to traditional disk-based systems. Spark can handle big data from various sources and formats, including structured, semi-structured, and unstructured data, making it a popular tool for big data processing and analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb81605",
   "metadata": {},
   "source": [
    "# WHY SPARK?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df025a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Spark is widely adopted due to several key benefits, including:\n",
    "\n",
    "Speed: Spark supports in-memory computing, which allows for faster processing of large data sets compared to traditional disk-based systems.\n",
    "\n",
    "Ease of Use: Spark has a high-level API in multiple programming languages, making it easy to develop and maintain applications.\n",
    "\n",
    "Scalability: Spark can be easily scaled up or down as needed, allowing for dynamic resource allocation and efficient use of hardware.\n",
    "\n",
    "Fault Tolerance: Spark automatically recovers lost data and re-executes failed operations, ensuring high availability and reliability.\n",
    "\n",
    "Support for Multiple Data Sources and Formats: Spark supports various data sources and formats, including structured, semi-structured, and unstructured data.\n",
    "\n",
    "Integrated with Other Big Data Tools: Spark integrates with other big data tools, such as Hadoop, and can run on top of existing Hadoop clusters.\n",
    "\n",
    "Overall, Spark provides a fast, easy-to-use, and flexible platform for big data processing and analysis.\n",
    "Spark does just that, managing and coordinating the execution of tasks on data across a cluster of computers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37959b54",
   "metadata": {},
   "source": [
    "# WHAT IS SPARK?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2337bcc4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Apache Spark is an open-source, distributed computing framework designed for large-scale data processing. It was developed to address the limitations of Hadoop MapReduce and provide a faster and more flexible solution for big data processing. Spark supports in-memory computing and offers APIs in multiple programming languages, making it easier to develop and maintain big data applications. Spark can process various types of data, including structured, semi-structured, and unstructured data, and integrates with other big data tools such as Hadoop and Apache Cassandra. Its advanced features and performance make Spark a popular choice for big data processing and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c1c35",
   "metadata": {},
   "source": [
    "# INTERNALS OF SPARK?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ecbfd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Resilient Distributed Datasets (RDD): Spark's fundamental data structure that allows it to distribute and parallelize data across multiple nodes in a cluster.\n",
    "\n",
    "Cluster Manager: Spark can run on a standalone cluster or on top of an existing cluster manager like Apache Mesos, Hadoop YARN or Kubernetes.\n",
    "\n",
    "Task Scheduler: The task scheduler is responsible for distributing tasks across the nodes in a cluster.\n",
    "\n",
    "Shuffle: The process of redistributing data across nodes for aggregation or joining purposes.\n",
    "\n",
    "DAG (Directed Acyclic Graph) Scheduler: The DAG scheduler builds a directed acyclic graph from RDD operations to optimize the task execution.\n",
    "\n",
    "Executors: Executors run on worker nodes and execute tasks assigned by the task scheduler.\n",
    "\n",
    "Driver Program: The driver program controls the execution of a Spark application and communicates with the executors.\n",
    "\n",
    "Caching: Spark can cache data in memory for faster access, reducing the need to recompute expensive operations.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e0f3a",
   "metadata": {},
   "source": [
    "# Highlevel API of spark? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc39eae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Sparksession\n",
    "Dataframe\n",
    "Partitions\n",
    "Transformation\n",
    "Actions\n",
    "Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b94d04",
   "metadata": {},
   "source": [
    "# Sparksession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57d4821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Assignment\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56e07c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.176.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Assignment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8814749ac0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53095c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRange = spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5cae0a",
   "metadata": {},
   "source": [
    "There is a SparkSession object available to the user, which is the entrance point to running Spark code.\n",
    "SparkSession is the entry point to programming Spark with the Dataset and DataFrame API. It is used for creating a Spark application, reading data and executing SQL queries. It provides a single point of access to all the Spark functionality, including creating RDDs, accumulators and broadcast variables, and starting new Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7eae74",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104251f",
   "metadata": {},
   "source": [
    "A DataFrame in Spark is a distributed collection of data organized into named columns. It is similar to a table in a relational database or a data frame in R or Python. Spark DataFrames provide a rich API for operating on the data and are optimized for big data processing. They support operations such as filtering, aggregation, and transformation and can be easily converted to RDDs, SQL tables, and other data structures for further processing. DataFrames also allow for interoperation with Spark SQL and can be saved to various storage systems like HDFS, S3, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb64ca",
   "metadata": {},
   "source": [
    "# Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45fef3",
   "metadata": {},
   "source": [
    "In Spark, a partition is a unit of parallelism. It is a logical division of data that is processed in parallel across a cluster of nodes. Each partition contains a portion of the data and operates independently of other partitions.\n",
    "\n",
    "Spark partitions the data in RDDs and DataFrames to allow for parallel processing. The number of partitions can be specified while creating RDDs or DataFrames, or it can be determined dynamically by Spark based on the cluster configuration. A larger number of partitions can lead to higher parallelism, but also increases overhead for task scheduling and coordination. On the other hand, a smaller number of partitions can limit parallelism and impact the performance of processing large datasets. Proper partitioning of data is important for optimizing Spark performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695a6d2",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64691b5",
   "metadata": {},
   "source": [
    "In Spark, transformations are operations performed on RDDs or DataFrames that produce a new RDD or DataFrame as a result. Transformations are executed lazily, which means they are not executed immediately when they are called, but only when an action is performed on the RDD or DataFrame. The following are some of the commonly used transformations in Spark:\n",
    "\n",
    "map: applies a function to each element in the RDD or DataFrame and returns a new RDD or DataFrame with the results.\n",
    "\n",
    "filter: returns a new RDD or DataFrame containing only the elements that satisfy a given condition.\n",
    "\n",
    "flatMap: is similar to map, but each input item can be mapped to zero or more output items.\n",
    "\n",
    "reduceByKey: aggregates the values of each key in an RDD of key-value pairs, using a user-defined reduce function.\n",
    "\n",
    "groupByKey: groups the values of each key in an RDD of key-value pairs.\n",
    "\n",
    "join: combines two RDDs or DataFrames based on a common key.\n",
    "\n",
    "union: combines two RDDs or DataFrames into a single RDD or DataFrame by appending their elements.\n",
    "\n",
    "These transformations allow for the creation of complex processing pipelines, making it possible to process large amounts of data in a distributed and scalable manner.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8d2fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where(\"number % 2 = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91d4e8",
   "metadata": {},
   "source": [
    "# Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6abb6",
   "metadata": {},
   "source": [
    "In Spark, actions are operations that return a result or trigger the computation of a RDD or DataFrame. They are the operations that materialize the results of a series of transformations and bring back data to the driver program. The following are some of the commonly used actions in Spark:\n",
    "\n",
    "count: returns the number of elements in an RDD or DataFrame.\n",
    "\n",
    "first: returns the first n elements of an RDD or DataFrame.\n",
    "\n",
    "take: returns the first n elements of an RDD or DataFrame as an array.\n",
    "\n",
    "reduce: aggregates the elements of an RDD or DataFrame using a user-defined function.\n",
    "\n",
    "collect: returns all the elements of an RDD or DataFrame to the driver program as an array.\n",
    "\n",
    "saveAsTextFile: writes the elements of an RDD or DataFrame to a text file.\n",
    "\n",
    "saveAsParquetFile: writes the elements of an RDD or DataFrame to a Parquet file.\n",
    "\n",
    "show: displays the first n elements of an RDD or DataFrame in a tabular format.\n",
    "\n",
    "These actions trigger the computation of all the transformations in the processing pipeline and produce the final result of a Spark application. They are crucial for retrieving the results of a Spark computation and making them available for further analysis or storage.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64d81382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb825636",
   "metadata": {},
   "source": [
    "# Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6ba04",
   "metadata": {},
   "source": [
    "Lazy evaluation is a evaluation strategy used in Spark where transformations are not executed immediately when they are called, but instead, the transformations are recorded and their execution is delayed until an action is performed.\n",
    "\n",
    "The main advantage of lazy evaluation is that it allows Spark to optimize the execution plan and avoid unnecessary computation. By deferring the computation, Spark can analyze the entire sequence of transformations and determine the most efficient way to execute them. This can lead to significant performance improvements, especially when dealing with large datasets.\n",
    "\n",
    "Lazy evaluation also allows Spark to cache intermediate results, which can be reused if the same transformations are performed multiple times. This avoids recomputing the intermediate results, which can save significant time and resources.\n",
    "\n",
    "In summary, lazy evaluation is a key feature of Spark that enables efficient processing of big data and improves the overall performance of Spark applications.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8461ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 20:09:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.9.13 (main, May 23 2022 22:01:06)\n",
      "Spark context Web UI available at http://172.18.176.1:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1675089578491).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.shell import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "883d953a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyspark' from '/home/agharamudhalvi/spark-3.3.0-bin-hadoop3/python/pyspark/__init__.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7183d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.shell import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d4677af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SQLContext,SparkConf,StorageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4f09c1",
   "metadata": {},
   "source": [
    "# Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf3371",
   "metadata": {},
   "source": [
    "You can monitor the progress of a job through the Spark web UI. The Spark UI is\n",
    "available on port 4040 of the driver node. If you are running in local mode, this will\n",
    "be http://localhost:4040."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29aab6",
   "metadata": {},
   "source": [
    "# END TO END EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "269899c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark\\\n",
    " .read\\\n",
    " .option(\"inferSchema\", \"true\")\\\n",
    " .option(\"header\", \"true\")\\\n",
    " .csv(\"data/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f0751e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark\\\n",
    " .read\\\n",
    " .option(\"inferSchema\", \"true\")\\\n",
    " .option(\"header\", \"true\")\\\n",
    " .csv(\"data/flight-data/csv/2015-summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b7cdff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb51cd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#52 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#52 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#90]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#50,ORIGIN_COUNTRY_NAME#51,count#52] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/agharamudhalvi/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d52d2da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54a1f98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca1d9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ee6293e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2774795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52b3886b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e713bf",
   "metadata": {},
   "source": [
    "# DataFrames and SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5179b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5820073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#50], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#50, 2), ENSURE_REQUIREMENTS, [id=#124]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#50], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#50] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/agharamudhalvi/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#50], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#50, 2), ENSURE_REQUIREMENTS, [id=#137]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#50], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#50] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/agharamudhalvi/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "dataFrameWay = flightData2015\\\n",
    " .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    " .count()\n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ea7c066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e60edc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e11a5b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.select(max(\"count\")).take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68a3e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5979679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff35759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3279dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015\\\n",
    " .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    " .sum(\"count\")\\\n",
    " .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    " .sort(desc(\"destination_total\"))\\\n",
    " .limit(5)\\\n",
    " .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78de54dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#152L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#50,destination_total#152L])\n",
      "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#50], functions=[sum(count#52)])\n",
      "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#50, 2), ENSURE_REQUIREMENTS, [id=#337]\n",
      "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#50], functions=[partial_sum(count#52)])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#50,count#52] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/agharamudhalvi/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015\\\n",
    " .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    " .sum(\"count\")\\\n",
    " .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    " .sort(desc(\"destination_total\"))\\\n",
    " .limit(5)\\\n",
    " .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b917716",
   "metadata": {},
   "source": [
    "# Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061e717",
   "metadata": {},
   "source": [
    "Structured Streaming is a high-level API for stream processing that became\n",
    "production-ready in Spark 2.2. With Structured Streaming, you can take the same\n",
    "operations that you perform in batch mode using Spark’s structured APIs and run\n",
    "them in a streaming fashion. This can reduce latency and allow for incremental pro‐\n",
    "cessing. The best thing about Structured Streaming is that it allows you to rapidly and\n",
    "quickly extract value out of streaming systems with virtually no code changes. It also\n",
    "makes it easy to conceptualize because you can write your batch job as a way to pro‐\n",
    "totype it and then you can convert it to a streaming job. The way all of this works is\n",
    "by incrementally processing that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94268de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"data/retail-data/by-day/*.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "316b4e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92aada80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   17450.0|{2011-09-20 05:30...|          71601.44|\n",
      "|      null|{2011-11-14 05:30...|          55316.08|\n",
      "|      null|{2011-11-07 05:30...|          42939.17|\n",
      "|      null|{2011-03-29 05:30...| 33521.39999999998|\n",
      "|      null|{2011-12-08 05:30...|31975.590000000007|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "staticDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "  .sum(\"total_cost\")\\\n",
    "  .sort(desc(\"sum(total_cost)\"))\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a27ffc",
   "metadata": {},
   "source": [
    "By default, the value is 200, but because there aren’t many executors on this machine,\n",
    "it’s worth reducing this to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f7db550",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b0046a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame = spark.readStream\\\n",
    "    .schema(staticSchema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"/data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730275d",
   "metadata": {},
   "source": [
    "whether our DataFrame is streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54186956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbae3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "  .sum(\"total_cost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89f036de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 20:10:54 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8fa9e044-7e60-40c4-8495-8d9ebef4eeb8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/30 20:10:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f882bc0a340>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"customer_purchases\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60e4742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------+\n",
      "|CustomerId|window|sum(total_cost)|\n",
      "+----------+------+---------------+\n",
      "+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM customer_purchases\n",
    "  ORDER BY `sum(total_cost)` DESC\n",
    "  \"\"\")\\\n",
    "  .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87242ce",
   "metadata": {},
   "source": [
    "Machine Learning and Advanced Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85dc3f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticDataFrame.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09d90d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "preppedDataFrame = staticDataFrame\\\n",
    "  .na.fill(0)\\\n",
    "  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n",
    "  .coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e936a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataFrame = preppedDataFrame\\\n",
    "  .where(\"InvoiceDate < '2011-07-01'\")\n",
    "testDataFrame = preppedDataFrame\\\n",
    "  .where(\"InvoiceDate >= '2011-07-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de28404c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "296006"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataFrame.count()\n",
    "testDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5fb4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer()\\\n",
    "  .setInputCol(\"day_of_week\")\\\n",
    "  .setOutputCol(\"day_of_week_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93cee90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    "  .setInputCol(\"day_of_week_index\")\\\n",
    "  .setOutputCol(\"day_of_week_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e23b7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    "  .setOutputCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2061063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    "  .setStages([indexer, encoder, vectorAssembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4253e8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "259b3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining = fittedPipeline.transform(trainDataFrame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "578872e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans()\\\n",
    " .setK(20)\\\n",
    " .setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9d82fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee11d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTest = fittedPipeline.transform(testDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7a621",
   "metadata": {},
   "source": [
    "# Lower-Level APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c796ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791fc0cc",
   "metadata": {},
   "source": [
    "# Overview of Structured Spark Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7204d09",
   "metadata": {},
   "source": [
    "\n",
    "For example, the following code does not perform addition in Scala\n",
    "or Python; it actually performs addition purely in Spark:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9def0ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[(number + 10): bigint]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(500).toDF(\"number\")\n",
    "df.select(df[\"number\"] + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358479b4",
   "metadata": {},
   "source": [
    "# Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb263592",
   "metadata": {},
   "source": [
    "A row is nothing more than a record of data. Each record in a DataFrame must be of\n",
    "type Row, as we can see when we collect the following DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bed8190d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e7bcc7",
   "metadata": {},
   "source": [
    "# Spark Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7832dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "b = ByteType()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d62b48",
   "metadata": {},
   "source": [
    "# Day 2(chapter 4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6beae3e",
   "metadata": {},
   "source": [
    "# 2. Basic Structured Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e5dc2",
   "metadata": {},
   "source": [
    "# 2.1 Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "582179c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\")\\\n",
    ".load(\"data/flight-data/json/2015-summary.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "09cba1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\")\\\n",
    ".load(\"data/flight-data/json/2015-summary.json\").schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e2c05075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    "  .load(\"data/flight-data/json/2015-summary.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae9d7e",
   "metadata": {},
   "source": [
    "# 2.2 Columns and Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34ff518b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'someColumnName'>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1e289ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "|    1|\n",
      "|   62|\n",
      "|  588|\n",
      "|   40|\n",
      "|    1|\n",
      "|  325|\n",
      "|   39|\n",
      "|   64|\n",
      "|    1|\n",
      "|   41|\n",
      "|   30|\n",
      "|    6|\n",
      "|    4|\n",
      "|  230|\n",
      "|    1|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b2f3d8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'((((someCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5ee51",
   "metadata": {},
   "source": [
    "Accessing a DataFrame’s columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7950b335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"data/flight-data/json/2015-summary.json\")\\\n",
    " .columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4983444",
   "metadata": {},
   "source": [
    "# 2.3 Records and Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "55edd4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73b2e5",
   "metadata": {},
   "source": [
    "# 2.4 Create Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2d3eabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9517472f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e343b37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12ce06",
   "metadata": {},
   "source": [
    "# 2.6 Creating Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "92c309da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(\"data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"dfTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9dd7527f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"some\", StringType(), True),\n",
    "  StructField(\"col\", StringType(), True),\n",
    "  StructField(\"names\", LongType(), False)\n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638cf4d",
   "metadata": {},
   "source": [
    "# 2.7 select and selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "46754af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f940b741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1e58a0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(\n",
    "    expr(\"DEST_COUNTRY_NAME\"),\n",
    "    col(\"DEST_COUNTRY_NAME\"),\n",
    "    column(\"DEST_COUNTRY_NAME\"))\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "18583fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24bde9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\"))\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "35b4ec26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5da48db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.selectExpr(\n",
    "  \"*\", # all original columns\n",
    "  \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c90b3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f5a77cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e37783",
   "metadata": {},
   "source": [
    "# 2.8 Adding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "431b9682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "36dcaa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n",
    "  .show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b500501",
   "metadata": {},
   "source": [
    "# 2.9 Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3cb2f9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5b82dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithLongColName = df.withColumn(\n",
    "    \"This Long Column-Name\",\n",
    "    expr(\"ORIGIN_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3cdb3b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-Name|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "|    United States|            Romania|   15|              Romania|\n",
      "|    United States|            Croatia|    1|              Croatia|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithLongColName.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ace57890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithLongColName.selectExpr(\n",
    "    \"`This Long Column-Name`\",\n",
    "    \"`This Long Column-Name` as `new col`\")\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c7e92433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This Long Column-Name']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c831fba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1ad1da54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count: bigint, This Long Column-Name: string]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1417e38",
   "metadata": {},
   "source": [
    "# 2.10 Changing column's type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0c231d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "020dcd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"count\",col(\"count\").cast(\"int\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef7984",
   "metadata": {},
   "source": [
    "# 2.11 Filtering rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0accd438",
   "metadata": {},
   "outputs": [],
   "source": [
    "colCondition = df.filter(col(\"count\") < 2).take(2)\n",
    "conditional = df.where(\"count < 2\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "40232c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]\n",
      "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]\n"
     ]
    }
   ],
   "source": [
    "print(colCondition)\n",
    "print(conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "10deb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed0481c",
   "metadata": {},
   "source": [
    "# 2.12 Getting unique rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3a40d7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b6b51ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94334c7",
   "metadata": {},
   "source": [
    "# 2.13 Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a408f217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"count\").show(5)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7d0ec5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "df.orderBy(desc(col(\"count\")), asc(col(\"DEST_COUNTRY_NAME\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "10b9c9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"data/flight-data/json/*-summary.json\")\\\n",
    "  .sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02e188",
   "metadata": {},
   "source": [
    "# 2.14 Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "96df692e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "500ca1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890a8741",
   "metadata": {},
   "source": [
    "# 2.15 Random Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "388ece2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c44f30",
   "metadata": {},
   "source": [
    "# 2.16 Random Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8c08324a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "dataFrames[0].count() > dataFrames[1].count() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f5742",
   "metadata": {},
   "source": [
    "# 2.17 Concatenating and Appending Rows (Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a93c2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "  Row(\"New Country\", \"Other Country\", 5),\n",
    "  Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "aa3d0e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.union(newDF)\\\n",
    "  .where(\"count = 1\")\\\n",
    "  .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82303a4",
   "metadata": {},
   "source": [
    "# 2.18 Repartition and Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f625d673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a06ec68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d8e1d47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "14c8c51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f7c239f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43c58b",
   "metadata": {},
   "source": [
    "# Collecting Rows to the Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671375ca",
   "metadata": {},
   "source": [
    "Spark maintains the state of the cluster in the\n",
    "driver. There are times when you’ll want to collect some of your data to the driver in\n",
    "order to manipulate it on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2bcd70bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "997d17bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _local_iterator_from_socket.<locals>.PyLocalIterable.__iter__ at 0x7f880f84fdd0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6aec0",
   "metadata": {},
   "source": [
    "# Working with Different Types of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7708e186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e44c043",
   "metadata": {},
   "source": [
    "# Converting to Spark Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0e7b4b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f267c81",
   "metadata": {},
   "source": [
    "# Working with Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2a8d8a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.where(col(\"InvoiceNo\") != 536365)\\\n",
    "  .select(\"InvoiceNo\", \"Description\")\\\n",
    "  .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "00e8e6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "70cd0e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|unitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",
    "  .where(\"isExpensive\")\\\n",
    "  .select(\"unitPrice\", \"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "847c6119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|   Description|UnitPrice|\n",
      "+--------------+---------+\n",
      "|DOTCOM POSTAGE|   569.77|\n",
      "|DOTCOM POSTAGE|   607.49|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\\\n",
    "  .where(\"isExpensive\")\\\n",
    "  .select(\"Description\", \"UnitPrice\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb15df",
   "metadata": {},
   "source": [
    "null-safe equivalence test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2d8cdf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"Description\").eqNullSafe(\"hello\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d45dcc",
   "metadata": {},
   "source": [
    "# Working with Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "75a2649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, pow\n",
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "58f14339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "  \"CustomerId\",\n",
    "  \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fd6d07ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, round, bround\n",
    "\n",
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c24e70ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "dedb667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4df2273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import count, mean, stddev_pop, min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "34170c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colName = \"UnitPrice\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b3c2879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 21:31:46 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "|             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21908|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22818|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|           15056BL|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             72817|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22545|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22988|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
      "|             22274|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             20750|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|            82616C|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21703|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22899|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
      "|             22379|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22422|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22769|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22585|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.crosstab(\"StockCode\", \"Quantity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3dae94c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "| StockCode_freqItems|  Quantity_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[90214E, 20728, 2...|[200, 128, 23, 32...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.freqItems([\"StockCode\", \"Quantity\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c8843b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.select(monotonically_increasing_id()).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dc98a8",
   "metadata": {},
   "source": [
    "# Working with String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3e5a7f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|initcap(Description)|\n",
      "+--------------------+\n",
      "|White Hanging Hea...|\n",
      "| White Metal Lantern|\n",
      "|Cream Cupid Heart...|\n",
      "|Knitted Union Fla...|\n",
      "|Red Woolly Hottie...|\n",
      "|Set 7 Babushka Ne...|\n",
      "|Glass Star Froste...|\n",
      "|Hand Warmer Union...|\n",
      "|Hand Warmer Red P...|\n",
      "|Assorted Colour B...|\n",
      "|Poppy's Playhouse...|\n",
      "|Poppy's Playhouse...|\n",
      "|Feltcraft Princes...|\n",
      "|Ivory Knitted Mug...|\n",
      "|Box Of 6 Assorted...|\n",
      "|Box Of Vintage Ji...|\n",
      "|Box Of Vintage Al...|\n",
      "|Home Building Blo...|\n",
      "|Love Building Blo...|\n",
      "|Recipe Box With M...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(col(\"Description\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3a91faf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------+\n",
      "|         Description|  lower(Description)|upper(lower(Description))|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "df.select(col(\"Description\"),\n",
    "    lower(col(\"Description\")),\n",
    "    upper(lower(col(\"Description\")))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3ed16a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+---+----------+\n",
      "|    ltrim|    rtrim| trim| lp|        rp|\n",
      "+---------+---------+-----+---+----------+\n",
      "|HELLO    |    HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO    |    HELLO|HELLO|HEL|HELLO     |\n",
      "+---------+---------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "    ltrim(lit(\"    HELLO    \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"    HELLO    \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"    HELLO    \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25861458",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "12238428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "df.select(\n",
    "  regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n",
    "  col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f1f27d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------+\n",
      "|translate(Description, LEET, 1337)|         Description|\n",
      "+----------------------------------+--------------------+\n",
      "|              WHI73 HANGING H3A...|WHITE HANGING HEA...|\n",
      "|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|\n",
      "+----------------------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"),col(\"Description\"))\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6a7a0430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|color_clean|         Description|\n",
      "+-----------+--------------------+\n",
      "|      WHITE|WHITE HANGING HEA...|\n",
      "|      WHITE| WHITE METAL LANTERN|\n",
      "+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(\n",
    "     regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n",
    "     col(\"Description\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "77794780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
    "  .where(\"hasSimpleColor\")\\\n",
    "  .select(\"Description\").show(3, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "43b015d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, locate\n",
    "simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "def color_locator(column, color_string):\n",
    "  return locate(color_string.upper(), column)\\\n",
    "          .cast(\"boolean\")\\\n",
    "          .alias(\"is_\" + color_string)\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "selectedColumns.append(expr(\"*\")) # has to a be Column type\n",
    "\n",
    "df.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n",
    "  .select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0474ed82",
   "metadata": {},
   "source": [
    "# Working with Dates and Timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "71eb2c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "04da1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDF = spark.range(10)\\\n",
    "  .withColumn(\"today\", current_date())\\\n",
    "  .withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2719ec04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2023-01-25|        2023-02-04|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dfc1de4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    "  .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n",
    "\n",
    "dateDF.select(\n",
    "    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    "  .select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c7989533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|to_date(date)|\n",
      "+-------------+\n",
      "|   2017-01-01|\n",
      "+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",
    "  .select(to_date(col(\"date\"))).show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "490cd5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b7fec229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|to_timestamp(date, yyyy-dd-MM)|\n",
      "+------------------------------+\n",
      "|           2017-11-12 00:00:00|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917892db",
   "metadata": {},
   "source": [
    "# Working with Nulls in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ba0c9",
   "metadata": {},
   "source": [
    "# Coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f89271",
   "metadata": {},
   "source": [
    "Spark includes a function to allow you to select the first non-null value from a set of\n",
    "columns by using the coalesce function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0310c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|coalesce(Description, CustomerId)|\n",
      "+---------------------------------+\n",
      "|             WHITE HANGING HEA...|\n",
      "|              WHITE METAL LANTERN|\n",
      "|             CREAM CUPID HEART...|\n",
      "|             KNITTED UNION FLA...|\n",
      "|             RED WOOLLY HOTTIE...|\n",
      "|             SET 7 BABUSHKA NE...|\n",
      "|             GLASS STAR FROSTE...|\n",
      "|             HAND WARMER UNION...|\n",
      "|             HAND WARMER RED P...|\n",
      "|             ASSORTED COLOUR B...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             FELTCRAFT PRINCES...|\n",
      "|             IVORY KNITTED MUG...|\n",
      "|             BOX OF 6 ASSORTED...|\n",
      "|             BOX OF VINTAGE JI...|\n",
      "|             BOX OF VINTAGE AL...|\n",
      "|             HOME BUILDING BLO...|\n",
      "|             LOVE BUILDING BLO...|\n",
      "|             RECIPE BOX WITH M...|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aff0f2",
   "metadata": {},
   "source": [
    "# ifnull, nullIf, nvl, and nvl2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87b5a0",
   "metadata": {},
   "source": [
    "There are several other SQL functions that you can use to achieve similar things.\n",
    "ifnull allows you to select the second value if the first is null, and defaults to the first. Alternatively, you could use nullif, which returns null if the two values are\n",
    "equal or else returns the second if they are not. nvl returns the second value if the\n",
    "first is null, but defaults to the first. Finally, nvl2 returns the second value if the first is\n",
    "not null; otherwise, it will return the last specified value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc30bee",
   "metadata": {},
   "source": [
    "# drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "acabb2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fd9d4",
   "metadata": {},
   "source": [
    "# fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8269580a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "439bef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_cols_vals = {\"StockCode\": 5, \"Description\" : \"No Value\"}\n",
    "df.na.fill(fill_cols_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122ff42",
   "metadata": {},
   "source": [
    "# replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1859a666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95ce4a",
   "metadata": {},
   "source": [
    "# Working with Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c78b46",
   "metadata": {},
   "source": [
    "# Structs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e8ff0154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[complex: struct<Description:string,InvoiceNo:string>, InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\")\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "03eecf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb29b58",
   "metadata": {},
   "source": [
    "# Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d1f98",
   "metadata": {},
   "source": [
    "# split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2ea09b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [WHITE, METAL, LA...|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "051031d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n",
    "  .selectExpr(\"array_col[0]\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bada6cb",
   "metadata": {},
   "source": [
    "# Array Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "298393dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "|                              3|\n",
      "+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f92e0d",
   "metadata": {},
   "source": [
    "# array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5df64e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6bdb94",
   "metadata": {},
   "source": [
    "# explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "518cfc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|         Description|InvoiceNo|exploded|\n",
      "+--------------------+---------+--------+\n",
      "|WHITE HANGING HEA...|   536365|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365| HANGING|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    "  .withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    "  .select(\"Description\", \"InvoiceNo\", \"exploded\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c291975",
   "metadata": {},
   "source": [
    "# Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "88727320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         complex_map|\n",
      "+--------------------+\n",
      "|{WHITE HANGING HE...|\n",
      "|{WHITE METAL LANT...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2511338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            null|\n",
      "|                          536365|\n",
      "+--------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "  .selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "369d4fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                 key| value|\n",
      "+--------------------+------+\n",
      "|WHITE HANGING HEA...|536365|\n",
      "| WHITE METAL LANTERN|536365|\n",
      "+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "  .selectExpr(\"explode(complex_map)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e1a0a",
   "metadata": {},
   "source": [
    "# Working with JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "554bba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "  '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4eb5efc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+--------------------+\n",
      "|get_json_object(jsonString, $.myJSONKey.myJSONValue[1])|                  c0|\n",
      "+-------------------------------------------------------+--------------------+\n",
      "|                                                      2|{\"myJSONValue\":[1...|\n",
      "+-------------------------------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\"),\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7003f444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[to_json(myStruct): string]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    "  .select(to_json(col(\"myStruct\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d5b73e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  from_json(newJSON)|             newJSON|\n",
      "+--------------------+--------------------+\n",
      "|{536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n",
      "|{536365, WHITE ME...|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "parseSchema = StructType((\n",
    "  StructField(\"InvoiceNo\",StringType(),True),\n",
    "  StructField(\"Description\",StringType(),True)))\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    "  .select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    "  .select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2aa0f2",
   "metadata": {},
   "source": [
    "# User-Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b2d8f0",
   "metadata": {},
   "source": [
    "The first step is the actual function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "903390f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "  return double_value ** 3\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98611b",
   "metadata": {},
   "source": [
    "Register the function to make it available as a DataFrame function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c2d79683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0054349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 212:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02460601",
   "metadata": {},
   "source": [
    "Register this UDF as a Spark SQL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "60d0aff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.power3(double_value)>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "spark.udf.register(\"power3py\", power3, DoubleType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "838d7214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|power3py(num)|\n",
      "+-------------+\n",
      "|         null|\n",
      "|         null|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF.selectExpr(\"power3py(num)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c24d87",
   "metadata": {},
   "source": [
    "# Day 3 (chapter 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c230e78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 219:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 22:17:08 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"data/retail-data/all/*.csv\")\\\n",
    "  .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15fe8d",
   "metadata": {},
   "source": [
    "# 1. Aggregation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7e003",
   "metadata": {},
   "source": [
    "# 1.1 count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "99186dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 220:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acaebbf",
   "metadata": {},
   "source": [
    "# 1.2 countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "cfb96629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 223:======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e81aee87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 229:======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1edc8",
   "metadata": {},
   "source": [
    "# 1.3 first and last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7f9bb218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f23d8",
   "metadata": {},
   "source": [
    "# 1.4 min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "22915890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f448966",
   "metadata": {},
   "source": [
    "# 1.5 sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bd7b4df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359e3a1",
   "metadata": {},
   "source": [
    "# 1.6 sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dbcc0c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e511f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    "  .selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790cc2f9",
   "metadata": {},
   "source": [
    "# 1.7 Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1f45af60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609056|47559.391409298754|  218.08095663447796|   218.08115785023418|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffba7f1",
   "metadata": {},
   "source": [
    "# 1.8 skewness and kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0195e700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052562|119768.05495536952|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0bead",
   "metadata": {},
   "source": [
    "# 1.9 Covariance and Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8a0b3046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 262:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085635685E-4|             1052.7280543902734|            1052.7260778741693|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "    covar_pop(\"InvoiceNo\", \"Quantity\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3aaf55",
   "metadata": {},
   "source": [
    "# 1.10 Aggregating to Complex Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "177154c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dbe293",
   "metadata": {},
   "source": [
    "# 1.11 Grouping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2547c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 268:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536370|  20|             20|\n",
      "|   536380|   1|              1|\n",
      "|   536384|  13|             13|\n",
      "|   536387|   5|              5|\n",
      "|   536397|   2|              2|\n",
      "|   536405|   1|              1|\n",
      "|   536407|   2|              2|\n",
      "|   536463|   1|              1|\n",
      "|   536500|  15|             15|\n",
      "|   536522|  54|             54|\n",
      "|   536523|  12|             12|\n",
      "|   536536|   3|              3|\n",
      "|   536538|  31|             31|\n",
      "|   536542|  16|             16|\n",
      "|   536555|   2|              2|\n",
      "|   536561|  15|             15|\n",
      "|   536573|   4|              4|\n",
      "|   536579|   2|              2|\n",
      "|   536580|   6|              6|\n",
      "|   536582|  17|             17|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a7d60b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 271:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536366|     17850|    2|\n",
      "|   536367|     13047|   12|\n",
      "|   536369|     13047|    1|\n",
      "|   536376|     15291|    2|\n",
      "|   536387|     16029|    5|\n",
      "|  C536391|     17548|    7|\n",
      "|   536392|     13705|   10|\n",
      "|   536399|     17850|    2|\n",
      "|   536403|     12791|    2|\n",
      "|   536405|     14045|    1|\n",
      "|   536415|     12838|   59|\n",
      "|   536446|     15983|   32|\n",
      "|   536463|     14849|    1|\n",
      "|   536464|     17968|   85|\n",
      "|   536500|     17377|   15|\n",
      "|   536525|     14078|   13|\n",
      "|   536529|     14237|    9|\n",
      "|   536531|     15485|   23|\n",
      "|   536532|     12433|   73|\n",
      "|   536533|     16955|    6|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145581b",
   "metadata": {},
   "source": [
    "# 1.12 Grouping with Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c1ba1801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 274:>                                                        (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536370|             22.45|   8.935742834258381|\n",
      "|   536380|              24.0|                 0.0|\n",
      "|   536384|14.615384615384615|  15.750645708563392|\n",
      "|   536387|             288.0|  117.57550765359255|\n",
      "|   536397|              30.0|                18.0|\n",
      "|   536405|             128.0|                 0.0|\n",
      "|   536407|               6.0|                 0.0|\n",
      "|   536463|              12.0|                 0.0|\n",
      "|   536500|               6.8|   4.019950248448356|\n",
      "|   536522|1.5925925925925926|  1.6046058535136642|\n",
      "|   536523| 9.333333333333334|   7.487025815072067|\n",
      "|   536536|31.666666666666668|  34.373762603991366|\n",
      "|   536538| 4.709677419354839|  3.7173833008743054|\n",
      "|   536542|              24.5|    8.73212459828649|\n",
      "|   536555|               1.0|                 0.0|\n",
      "|   536561| 9.333333333333334|  2.9814239699997196|\n",
      "|   536573|              34.0|  23.748684174075834|\n",
      "|   536579|             138.0|                78.0|\n",
      "|   536580|              11.0|    2.23606797749979|\n",
      "|   536582|10.823529411764707|    6.99777522727673|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e3b42929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, to_date\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a41f15",
   "metadata": {},
   "source": [
    "# 1.13 Grouping with Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "671a5ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536370|             22.45|   8.935742834258381|\n",
      "|   536380|              24.0|                 0.0|\n",
      "|   536384|14.615384615384615|  15.750645708563392|\n",
      "|   536387|             288.0|  117.57550765359255|\n",
      "|   536397|              30.0|                18.0|\n",
      "|   536405|             128.0|                 0.0|\n",
      "|   536407|               6.0|                 0.0|\n",
      "|   536463|              12.0|                 0.0|\n",
      "|   536500|               6.8|   4.019950248448356|\n",
      "|   536522|1.5925925925925926|  1.6046058535136642|\n",
      "|   536523| 9.333333333333334|   7.487025815072067|\n",
      "|   536536|31.666666666666668|  34.373762603991366|\n",
      "|   536538| 4.709677419354839|  3.7173833008743054|\n",
      "|   536542|              24.5|    8.73212459828649|\n",
      "|   536555|               1.0|                 0.0|\n",
      "|   536561| 9.333333333333334|  2.9814239699997196|\n",
      "|   536573|              34.0|  23.748684174075834|\n",
      "|   536579|             138.0|                78.0|\n",
      "|   536580|              11.0|    2.23606797749979|\n",
      "|   536582|10.823529411764707|    6.99777522727673|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n",
    " .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cc82c",
   "metadata": {},
   "source": [
    "# 1.14 Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "824e1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "781479a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "windowSpec = Window\\\n",
    "  .partitionBy(\"CustomerId\", \"date\")\\\n",
    "  .orderBy(desc(\"Quantity\"))\\\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c3a2aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "57315c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c2df7957",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    "  .select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantityRank\"),\n",
    "    purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e84ec",
   "metadata": {},
   "source": [
    "# 1.15 Grouping Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "091eddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611a15e",
   "metadata": {},
   "source": [
    "# 1.16 Rollups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d1365587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 287:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|          null|       5176450|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|          null|         26814|\n",
      "|2010-12-01|United Kingdom|         23949|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "|2010-12-02|          null|         21023|\n",
      "|2010-12-02|       Germany|           146|\n",
      "|2010-12-02|United Kingdom|         20873|\n",
      "|2010-12-02|          EIRE|             4|\n",
      "|2010-12-03|       Belgium|           528|\n",
      "|2010-12-03|   Switzerland|           110|\n",
      "|2010-12-03|          EIRE|          2575|\n",
      "|2010-12-03|         Spain|           400|\n",
      "|2010-12-03|       Germany|           170|\n",
      "|2010-12-03|        France|           239|\n",
      "|2010-12-03|      Portugal|            65|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    "  .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\\\n",
    "  .orderBy(\"Date\")\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "53b90602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 290:======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|      Date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      null|   null|       5176450|\n",
      "|2010-12-01|   null|         26814|\n",
      "|2010-12-02|   null|         21023|\n",
      "|2010-12-03|   null|         14830|\n",
      "|2010-12-05|   null|         16395|\n",
      "|2010-12-06|   null|         21419|\n",
      "|2010-12-07|   null|         24995|\n",
      "|2010-12-08|   null|         22741|\n",
      "|2010-12-09|   null|         18431|\n",
      "|2010-12-10|   null|         20297|\n",
      "|2010-12-12|   null|         10565|\n",
      "|2010-12-13|   null|         17623|\n",
      "|2010-12-14|   null|         20098|\n",
      "|2010-12-15|   null|         18229|\n",
      "|2010-12-16|   null|         29632|\n",
      "|2010-12-17|   null|         16069|\n",
      "|2010-12-19|   null|          3795|\n",
      "|2010-12-20|   null|         14965|\n",
      "|2010-12-21|   null|         15467|\n",
      "|2010-12-22|   null|          3192|\n",
      "+----------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d66b54b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 293:======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+\n",
      "|Date|Country|total_quantity|\n",
      "+----+-------+--------------+\n",
      "|null|   null|       5176450|\n",
      "+----+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Date IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96af4212",
   "metadata": {},
   "source": [
    "# 1.17 Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "98b9f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 296:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-------------+\n",
      "|Date|        Country|sum(Quantity)|\n",
      "+----+---------------+-------------+\n",
      "|null|        Finland|        10666|\n",
      "|null| Czech Republic|          592|\n",
      "|null|      Singapore|         5234|\n",
      "|null|         Israel|         4353|\n",
      "|null|        Lebanon|          386|\n",
      "|null|         Norway|        19247|\n",
      "|null|        Germany|       117448|\n",
      "|null|Channel Islands|         9479|\n",
      "|null|           EIRE|       142637|\n",
      "|null|         Greece|         1556|\n",
      "|null|    Switzerland|        30325|\n",
      "|null|        Belgium|        23152|\n",
      "|null|         Poland|         3653|\n",
      "|null|        Denmark|         8188|\n",
      "|null|       Portugal|        16180|\n",
      "|null|      Hong Kong|         4769|\n",
      "|null|      Lithuania|          652|\n",
      "|null|          Italy|         7999|\n",
      "|null|      Australia|        83653|\n",
      "|null|          Spain|        26824|\n",
      "+----+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\")))\\\n",
    "  .select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0c4e0",
   "metadata": {},
   "source": [
    "# 1.18 Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "79908dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65fc701",
   "metadata": {},
   "source": [
    "# Day 4 (chapter 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97b5a4",
   "metadata": {},
   "source": [
    "# 1.JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "16bff059",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ1=spark.read.csv(\"data/flight-data/csv/2010-summary.csv\",header=True,inferSchema=True)\n",
    "summ2=spark.read.csv(\"data/flight-data/csv/2011-summary.csv\",header=True,inferSchema=True)\n",
    "summ3=spark.read.csv(\"data/flight-data/csv/2012-summary.csv\",header=True,inferSchema=True)\n",
    "summ4=spark.read.csv(\"data/flight-data/csv/2013-summary.csv\",header=True,inferSchema=True)\n",
    "summ5=spark.read.csv(\"data/flight-data/csv/2014-summary.csv\",header=True,inferSchema=True)\n",
    "summ6=spark.read.csv(\"data/flight-data/csv/2015-summary.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "0103a3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n",
      "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n",
      "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n",
      "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n",
      "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n",
      "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n"
     ]
    }
   ],
   "source": [
    "print(summ1.columns)\n",
    "print(summ2.columns)\n",
    "print(summ3.columns)\n",
    "print(summ4.columns)\n",
    "print(summ5.columns)\n",
    "print(summ6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f18ecc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    "  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    "  .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")])\\\n",
    "  .toDF(\"id\", \"status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ccfc3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a2a11986",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongJoinExpression = person[\"name\"] == graduateProgram[\"school\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "47177fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"inner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "053910ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "    (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\n",
    "\n",
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f09207d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personId|            name|graduate_program|   spark_status| id|        status|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "person.withColumnRenamed(\"id\", \"personId\")\\\n",
    "  .join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9bfba8",
   "metadata": {},
   "source": [
    "# Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "798fd142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 312:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "8dadc55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+--------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+--------------------+-----+\n",
      "|    United States|            Romania|    1|               Haiti|  197|\n",
      "|    United States|            Romania|    1|       French Guiana|   11|\n",
      "|    United States|            Romania|    1|Saint Kitts and N...|  120|\n",
      "|    United States|            Romania|    1| Trinidad and Tobago|  213|\n",
      "|    United States|            Romania|    1|             Bolivia|   51|\n",
      "+-----------------+-------------------+-----+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inner_join = summ1.join(summ2, [\"DEST_COUNTRY_NAME\"], \"inner\")\n",
    "inner_join.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fdf284",
   "metadata": {},
   "source": [
    "# Outer Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "614f6639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----+-------------------+-----+\n",
      "|  DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-------------------+-----+-------------------+-----+\n",
      "|        Afghanistan|      United States|   11|               null| null|\n",
      "|            Algeria|               null| null|      United States|    9|\n",
      "|             Angola|      United States|   14|      United States|   13|\n",
      "|           Anguilla|      United States|   21|      United States|   34|\n",
      "|Antigua and Barbuda|      United States|  123|      United States|  115|\n",
      "+-------------------+-------------------+-----+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_outer_join = summ1.join(summ5, [\"DEST_COUNTRY_NAME\"], \"full_outer\")\n",
    "full_outer_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f1a2be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf9bbb6",
   "metadata": {},
   "source": [
    "# Left Outer Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "ca1fffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+--------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+--------------------+-----+\n",
      "|    United States|            Croatia|    1|               Haiti|  186|\n",
      "|    United States|            Croatia|    1|       French Guiana|    3|\n",
      "|    United States|            Croatia|    1|Saint Kitts and N...|  115|\n",
      "|    United States|            Croatia|    1|             Bolivia|   13|\n",
      "|    United States|            Croatia|    1| Trinidad and Tobago|  184|\n",
      "+-----------------+-------------------+-----+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left_outer_join = summ3.join(summ4, [\"DEST_COUNTRY_NAME\"], \"left_outer\")\n",
    "left_outer_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d1341b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n",
      "|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17897d75",
   "metadata": {},
   "source": [
    "# Right Outer Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "fde73968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-----+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+--------------------+-----+-------------------+-----+\n",
      "|    United States|               Haiti|  193|            Romania|   15|\n",
      "|    United States|Saint Kitts and N...|  123|            Romania|   15|\n",
      "|    United States|       French Guiana|    4|            Romania|   15|\n",
      "|    United States|             Bolivia|   14|            Romania|   15|\n",
      "|    United States| Trinidad and Tobago|  175|            Romania|   15|\n",
      "+-----------------+--------------------+-----+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right_outer_join = summ5.join(summ6, [\"DEST_COUNTRY_NAME\"], \"right_outer\")\n",
    "right_outer_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "319804ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164d7d1",
   "metadata": {},
   "source": [
    "# Left Semi Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "198d436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_semi\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "758d3583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|    United States|          Singapore|   25|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left_semi_join = summ1.join(summ3, summ1.DEST_COUNTRY_NAME == summ3.DEST_COUNTRY_NAME, \"leftsemi\")\n",
    "left_semi_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "4acd9da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|\n",
      "|  0|Masters|      Duplicated Row|Duplicated School|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    " (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\n",
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")\n",
    "gradProgram2.join(person, joinExpression, joinType).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050a77a",
   "metadata": {},
   "source": [
    "# Left Anti Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "320d7f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_anti\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "c3bfea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|            Malta|      United States|    1|\n",
      "|            Yemen|      United States|    1|\n",
      "|       The Gambia|      United States|    1|\n",
      "|           Guinea|      United States|    5|\n",
      "|          Croatia|      United States|    2|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left_anti_join = summ2.join(summ4, summ2.DEST_COUNTRY_NAME == summ4.DEST_COUNTRY_NAME, \"leftanti\")\n",
    "left_anti_join.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40002b46",
   "metadata": {},
   "source": [
    "# Cross (Cartesian) Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "68e6413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+-----------------+-------------------+-----+\n",
      "|    United States|       Saint Martin|    2|    United States|       Saint Martin|    1|\n",
      "|    United States|       Saint Martin|    2|    United States|            Romania|   12|\n",
      "|    United States|       Saint Martin|    2|    United States|            Croatia|    2|\n",
      "|    United States|       Saint Martin|    2|    United States|            Ireland|  291|\n",
      "|    United States|       Saint Martin|    2|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_join = summ2.crossJoin(summ5)\n",
    "cross_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "422cd369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7bfb5c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.crossJoin(graduateProgram).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a91e7",
   "metadata": {},
   "source": [
    "# Challenges When Using Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d66c85",
   "metadata": {},
   "source": [
    "# Joins on Complex Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5112e63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personId|            name|graduate_program|   spark_status| id|        status|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "person.withColumnRenamed(\"id\", \"personId\")\\\n",
    " .join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef13424",
   "metadata": {},
   "source": [
    "# Handling Duplicate Column Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad181e5",
   "metadata": {},
   "source": [
    "1: Renaming the columns: You can use the withColumnRenamed method to rename the duplicate columns before the join. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "6b21cb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME_1|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-------------------+-----+\n",
      "|      United States|            Romania|    1|\n",
      "|      United States|            Ireland|  264|\n",
      "|      United States|              India|   69|\n",
      "|              Egypt|      United States|   24|\n",
      "|  Equatorial Guinea|      United States|    1|\n",
      "+-------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME_2|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-------------------+-----+\n",
      "|      United States|       Saint Martin|    2|\n",
      "|      United States|             Guinea|    2|\n",
      "|      United States|            Croatia|    1|\n",
      "|      United States|            Romania|    3|\n",
      "|      United States|            Ireland|  268|\n",
      "+-------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summ1.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"DEST_COUNTRY_NAME_1\").show(5)\n",
    "summ2.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"DEST_COUNTRY_NAME_2\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5d7ab",
   "metadata": {},
   "source": [
    "2: Using the as keyword: When selecting columns, you can use the alias method or the as keyword to give a new name to the duplicate column. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f0f4c598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|DEST_COUNTRY_NAME_1|\n",
      "+-------------------+\n",
      "|      United States|\n",
      "|      United States|\n",
      "|      United States|\n",
      "|              Egypt|\n",
      "|  Equatorial Guinea|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+\n",
      "|DEST_COUNTRY_NAME_1|\n",
      "+-------------------+\n",
      "|      United States|\n",
      "|      United States|\n",
      "|      United States|\n",
      "|      United States|\n",
      "|      United States|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summ1.select(summ1[\"DEST_COUNTRY_NAME\"].alias(\"DEST_COUNTRY_NAME_1\")).show(5)\n",
    "summ2.selectExpr(\"DEST_COUNTRY_NAME as DEST_COUNTRY_NAME_1\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924f100",
   "metadata": {},
   "source": [
    "3: Using the withColumn method: You can use the withColumn method to add a new column with a new name, and then drop the original column. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "500ea413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------------------+\n",
      "|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME_1|\n",
      "+-------------------+-----+-------------------+\n",
      "|            Romania|    1|      United States|\n",
      "|            Ireland|  264|      United States|\n",
      "|              India|   69|      United States|\n",
      "|      United States|   24|              Egypt|\n",
      "|      United States|    1|  Equatorial Guinea|\n",
      "+-------------------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summ= summ1.withColumn(\"DEST_COUNTRY_NAME_1\", summ1[\"DEST_COUNTRY_NAME\"])\n",
    "summ.drop(\"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97476090",
   "metadata": {},
   "source": [
    "4: Using the select method: You can use the select method to select only the columns you need from the DataFrame, which will remove the duplicate columns. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "599de401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------------------+\n",
      "|DEST_COUNTRY_NAME_1|count|ORIGIN_COUNTRY_NAME|\n",
      "+-------------------+-----+-------------------+\n",
      "|      United States|    1|            Romania|\n",
      "|      United States|  264|            Ireland|\n",
      "|      United States|   69|              India|\n",
      "|              Egypt|   24|      United States|\n",
      "|  Equatorial Guinea|    1|      United States|\n",
      "+-------------------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summ.select(\"DEST_COUNTRY_NAME_1\",\"count\",\"ORIGIN_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5f775",
   "metadata": {},
   "source": [
    "5: Using the drop method: You can use the drop method to drop duplicate columns after join the dataframe. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "65cbbd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-------------------+--------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME_1| ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+-------------------+--------------------+-----+\n",
      "|            Romania|    1|      United States|               Haiti|  197|\n",
      "|            Romania|    1|      United States|       French Guiana|   11|\n",
      "|            Romania|    1|      United States|Saint Kitts and N...|  120|\n",
      "|            Romania|    1|      United States| Trinidad and Tobago|  213|\n",
      "|            Romania|    1|      United States|             Bolivia|   51|\n",
      "+-------------------+-----+-------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum = summ.join(summ2, summ1.DEST_COUNTRY_NAME == summ2.DEST_COUNTRY_NAME, \"inner\")\n",
    "sum.drop(\"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a54fa6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef34557",
   "metadata": {},
   "source": [
    "# Approach 1: Different join expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff5a96",
   "metadata": {},
   "source": [
    "When you have two keys that have the same name, probably the easiest fix is to change the join expression from a Boolean expression to a string or sequence. This automatically removes one of the columns for you during the join:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea986b6",
   "metadata": {},
   "source": [
    "# Approach 2: Dropping the column after the join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb894903",
   "metadata": {},
   "source": [
    "Another approach is to drop the offending column after the join. When doing this,\n",
    "we need to refer to the column via the original source DataFrame. We can do this if\n",
    "the join uses the same key names or if the source DataFrames have columns that sim‐\n",
    "ply have the same name:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae8b9f9",
   "metadata": {},
   "source": [
    "# Approach 3: Renaming a column before the join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622f0a6",
   "metadata": {},
   "source": [
    "We can avoid this issue altogether if we rename one of our columns before the join:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46936317",
   "metadata": {},
   "source": [
    "# 3. How spark performs joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3aa49",
   "metadata": {},
   "source": [
    "In PySpark, joins are performed by the join method on a DataFrame, which takes one or more DataFrames as arguments. The basic syntax for joining two DataFrames is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "8e14c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 23:24:57 WARN Column: Constructing trivially true equals predicate, 'DEST_COUNTRY_NAME#12044 = DEST_COUNTRY_NAME#12044'. Perhaps you need to use aliases.\n",
      "+-----------------+-------------------+-----+-----------------+--------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+-----------------+--------------------+-----+\n",
      "|    United States|            Romania|    1|    United States|              Uganda|    1|\n",
      "|    United States|            Romania|    1|    United States|               Haiti|  226|\n",
      "|    United States|            Romania|    1|    United States|       French Guiana|    1|\n",
      "|    United States|            Romania|    1|    United States|Saint Kitts and N...|  127|\n",
      "|    United States|            Romania|    1|    United States|            Slovakia|    1|\n",
      "+-----------------+-------------------+-----+-----------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summ1.join(summ1, summ1.DEST_COUNTRY_NAME == summ1.DEST_COUNTRY_NAME, \"inner\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7c6c0e",
   "metadata": {},
   "source": [
    "When the join method is called, Spark will perform the following steps:\n",
    "\n",
    "Broadcast the smaller DataFrame: If one of the DataFrames is smaller than the other, Spark will broadcast it to all the worker nodes so that it can be used for the join.\n",
    "\n",
    "Partition the larger DataFrame: The larger DataFrame is partitioned into smaller chunks called RDDs, which are distributed across the worker nodes.\n",
    "\n",
    "Shuffle the data: The data is shuffled so that all the rows with the same join key are on the same worker node. This step is necessary so that the join can be performed in parallel.\n",
    "\n",
    "Perform the join: Each worker node performs the join locally on its partition of the data. The join is performed based on the join condition specified in the join method.\n",
    "\n",
    "Collect the results: The results from all the worker nodes are collected and combined to form the final joined DataFrame.\n",
    "\n",
    "It's important to note that the performance of the join operation depends on the distribution of the data and the size of the DataFrames. If the data is not well-distributed, a large amount of data may need to be shuffled, which can cause performance issues. Additionally, if the DataFrames are very large, it may be more efficient to perform a broadcast or bucketed join, or to use a different join strategy such as map-side join."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3883c2",
   "metadata": {},
   "source": [
    "# Day 5 (chapter 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381be38",
   "metadata": {},
   "source": [
    "# 1. Datasources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb64bc",
   "metadata": {},
   "source": [
    "# 1.1. Basics of reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb24480",
   "metadata": {},
   "source": [
    "The most commonly used method for reading data in PySpark is the read method of the SparkSession object.\n",
    "\n",
    "Here is an example of how to read a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "26581e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum1 = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3d36a",
   "metadata": {},
   "source": [
    "In this example, we first create a SparkSession object, and then use the read.csv method to read the CSV file located at the specified path. The header parameter is set to True so that the first row of the CSV file is used as the header, and the inferSchema parameter is set to True so that PySpark can infer the data types of the columns.\n",
    "\n",
    "You can also read data from other file formats like json, parquet etc by using spark.read.json(), spark.read.parquet() respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab1260",
   "metadata": {},
   "source": [
    "# 2. Basics of write data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701da4e5",
   "metadata": {},
   "source": [
    "The most commonly used method for writing data in PySpark is the write method of the DataFrame object.\n",
    "\n",
    "Here is an example of how to write a DataFrame to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a8b1966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.csv(\"path/to/new_file.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0be99",
   "metadata": {},
   "source": [
    "In this example, we first create a SparkSession object, and then use the read.csv method to read the CSV file located at the specified path. Then we use the write.csv method to write the DataFrame to a new CSV file located at the specified path. The header parameter is set to True so that the column names will be written as the first row of the new CSV file.\n",
    "\n",
    "You can also write data to other file formats like json, parquet etc by using df.write.json(), df.write.parquet() respectively. Additionally you can also write data to various databases like hive, mysql etc using df.write.jdbc() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681bdc5",
   "metadata": {},
   "source": [
    "# 3. CSV files - reading, writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e02ab3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"mode\", \"FAILFAST\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"data/flight-data/csv/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f8069686",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\\\n",
    "  .save(\"/tmp/my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3bc55",
   "metadata": {},
   "source": [
    "# 4. JSON files - reading, writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "caf696d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"data/flight-data/json/2010-summary.json\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "e72e0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"json\").mode(\"overwrite\").save(\"/tmp/my-json-file.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3fb81d",
   "metadata": {},
   "source": [
    "# 5. Parquet files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5aa77",
   "metadata": {},
   "source": [
    "Parquet is a columnar storage format that is widely used in the Apache Hadoop ecosystem and is supported by many big data processing frameworks, including PySpark. There are several reasons why Parquet is important in PySpark:\n",
    "\n",
    "Efficiency: Parquet stores data in a columnar format, which means that only the required columns are read and processed, rather than reading and processing the entire row. This leads to significant performance improvements when working with large datasets.\n",
    "\n",
    "Compression: Parquet supports various compression algorithms, such as Snappy and Gzip, which can greatly reduce the storage space required for large datasets.\n",
    "\n",
    "Schema evolution: Parquet supports schema evolution, which means that a dataset's schema can be changed over time without having to rewrite the entire dataset. This is particularly useful when working with data that is constantly changing or evolving.\n",
    "\n",
    "Predicate pushdown: Parquet supports predicate pushdown, which means that filtering conditions can be pushed down to the storage layer, rather than being applied in the query layer. This leads to further performance improvements when working with large datasets.\n",
    "\n",
    "Interoperability: Parquet is an open standard, which means that it can be used with a wide variety of data processing frameworks, including PySpark, Hive, Pig, and Impala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be779083",
   "metadata": {},
   "source": [
    "# 6. PARQUET files - reading, writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba6e18",
   "metadata": {},
   "source": [
    "To read and write parquet files in PySpark you can use the read.parquet() and write.parquet() methods respectively. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "94c84004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\")\\\n",
    "  .load(\"data/flight-data/parquet/2010-summary.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "de10f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    "  .save(\"/tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c02ccf",
   "metadata": {},
   "source": [
    "# 7. ORC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb659a",
   "metadata": {},
   "source": [
    "ORC (Optimized Row Columnar) is a file format that is similar to Parquet and is also widely used in the Apache Hadoop ecosystem. Like Parquet, ORC is a columnar storage format that is designed to improve the performance and storage efficiency of big data processing frameworks, such as PySpark.\n",
    "\n",
    "Here are some of the key benefits of using ORC in PySpark:\n",
    "\n",
    "Performance: ORC stores data in a columnar format, which leads to significant performance improvements when working with large datasets.\n",
    "\n",
    "Compression: ORC supports various compression algorithms, such as Snappy, Zlib, and LZO, which can greatly reduce the storage space required for large datasets.\n",
    "\n",
    "Schema evolution: ORC supports schema evolution, which means that a dataset's schema can be changed over time without having to rewrite the entire dataset.\n",
    "\n",
    "Predicate pushdown: ORC supports predicate pushdown, which means that filtering conditions can be pushed down to the storage layer, rather than being applied in the query layer.\n",
    "\n",
    "Interoperability: ORC is an open standard, which means that it can be used with a wide variety of data processing frameworks, including PySpark, Hive, Pig, and Impala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f726e6",
   "metadata": {},
   "source": [
    "# 8.ORC files - reading, writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "6f0f2462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"orc\").load(\"data/flight-data/orc/2010-summary.orc\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "6179d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"orc\").mode(\"overwrite\").save(\"/tmp/my-json-file.orc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef5b01",
   "metadata": {},
   "source": [
    "# 9. Reading from SQL Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "b176f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"org.sqlite.JDBC\"\n",
    "path = \"data/flight-data/jdbc/my-sqlite.db\"\n",
    "url = \"jdbc:sqlite:\" + path\n",
    "tablename = \"flight_info\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "eae75fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbDataFrame = spark.read.format(\"jdbc\").option(\"url\", url)\\\n",
    "  .option(\"dbtable\", tablename).option(\"driver\",  driver).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38996013",
   "metadata": {},
   "source": [
    "# 10. Query Pushdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdbfaa2",
   "metadata": {},
   "source": [
    "Spark makes a best-effort attempt to filter data in the database itself before cre‐\n",
    "ating the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "ee3d65e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(flight_info) [numPartitions=1] [DEST_COUNTRY_NAME#12763,ORIGIN_COUNTRY_NAME#12764,count#12765] PushedFilters: [*In(DEST_COUNTRY_NAME, [Anguilla,Sweden])], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:decimal(20,0)>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.filter(\"DEST_COUNTRY_NAME in ('Anguilla', 'Sweden')\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "969f0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushdownQuery = \"\"\"(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info)\n",
    "  AS flight_info\"\"\"\n",
    "dbDataFrame = spark.read.format(\"jdbc\")\\\n",
    "  .option(\"url\", url).option(\"dbtable\", pushdownQuery).option(\"driver\",  driver)\\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c70880",
   "metadata": {},
   "source": [
    "# 11. Reading from databases in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "51900be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbDataFrame = spark.read.format(\"jdbc\")\\\n",
    "  .option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\",  driver)\\\n",
    "  .option(\"numPartitions\", 10).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "bae73522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|           Sweden|      United States|   65|\n",
      "|    United States|             Sweden|   73|\n",
      "|         Anguilla|      United States|   21|\n",
      "|    United States|           Anguilla|   20|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props = {\"driver\":\"org.sqlite.JDBC\"}\n",
    "predicates = [\n",
    "  \"DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'\",\n",
    "  \"DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'\"]\n",
    "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).show()\n",
    "spark.read.jdbc(url,tablename,predicates=predicates,properties=props)\\\n",
    "  .rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2f2fb66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props = {\"driver\":\"org.sqlite.JDBC\"}\n",
    "predicates = [\n",
    "  \"DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'\",\n",
    "  \"DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'\"]\n",
    "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32145444",
   "metadata": {},
   "source": [
    "# 12.Partitioning based on a sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "de061314",
   "metadata": {},
   "outputs": [],
   "source": [
    "colName = \"count\"\n",
    "lowerBound = 0\n",
    "upperBound = 348113 # this is the max count in our database\n",
    "numPartitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "1e8ea34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.read.jdbc(url, tablename, column=colName, properties=props,\n",
    "                lowerBound=lowerBound, upperBound=upperBound,\n",
    "                numPartitions=numPartitions).count() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9daab8",
   "metadata": {},
   "source": [
    "# 13.Writing to SQL Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "5b023d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 10:49:41 WARN JdbcUtils: Requested isolation level 1 is not supported; falling back to default isolation level 8\n"
     ]
    }
   ],
   "source": [
    "newPath = \"jdbc:sqlite://tmp/my-sqlite.db\"\n",
    "csvFile.write.jdbc(newPath, tablename, mode=\"overwrite\", properties=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "5b611068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(newPath, tablename, properties=props).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d40b657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 10:49:53 WARN JdbcUtils: Requested isolation level 1 is not supported; falling back to default isolation level 8\n"
     ]
    }
   ],
   "source": [
    "csvFile.write.jdbc(newPath, tablename, mode=\"append\", properties=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c86bb4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(newPath, tablename, properties=props).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699ddf8",
   "metadata": {},
   "source": [
    "# 14.Partitioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b50100",
   "metadata": {},
   "source": [
    "Partitioning is a tool that allows you to control what data is stored (and where) as you\n",
    "write it. When you write a file to a partitioned directory (or table), you basically\n",
    "encode a column as a folder. What this allows you to do is skip lots of data when you\n",
    "go to read it in later, allowing you to read in only the data relevant to your problem\n",
    "instead of having to scan the complete dataset. These are supported for all file-based\n",
    "data sources:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "852811aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\\\n",
    " .save(\"/tmp/partitioned-files.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6590635",
   "metadata": {},
   "source": [
    "# 15.Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b938a024",
   "metadata": {},
   "source": [
    "Bucketing is another file organization approach with which you can control the data\n",
    "that is specifically written to each file. This can help avoid shuffles later when you go\n",
    "to read the data because data with the same bucket ID will all be grouped together\n",
    "into one physical partition.This means that the data is prepartitioned according to how you expect to use that data later on, meaning you can avoid expensive shuffles\n",
    "when joining or aggregating.\n",
    "Rather than partitioning on a specific column (which might write out a ton of direc‐\n",
    "tories), it’s probably worthwhile to explore bucketing the data instead. This will create\n",
    "a certain number of files and organize our data into those “buckets”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "f0f36208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "numberBuckets = 10\n",
    "columnToBucketBy = \"count\"\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    " .bucketBy(numberBuckets, columnToBucketBy).saveAsTable(\"bucketedFiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56f1cd5",
   "metadata": {},
   "source": [
    "# SPARK SQL (chapter 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53c0f8",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "9f857b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.json(\"data/flight-data/json/2015-summary.json\")\\\n",
    "  .createOrReplaceTempView(\"summary\") # DF => SQL\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\\\n",
    "  .where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "b8407d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM summary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdfe5fc",
   "metadata": {},
   "source": [
    "1.What is the total number of flights?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "9ad3977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|ROWS_count|\n",
      "+----------+\n",
      "|       256|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(*) AS ROWS_count FROM summary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ca927a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.What are the top 10 destination countries by count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "bd210dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "| DEST_COUNTRY_NAME|total_count|\n",
      "+------------------+-----------+\n",
      "|     United States|     411352|\n",
      "|            Canada|       8399|\n",
      "|            Mexico|       7140|\n",
      "|    United Kingdom|       2025|\n",
      "|             Japan|       1548|\n",
      "|           Germany|       1468|\n",
      "|Dominican Republic|       1353|\n",
      "|       South Korea|       1048|\n",
      "|       The Bahamas|        955|\n",
      "|            France|        935|\n",
      "+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
    "          FROM summary\\\n",
    "          GROUP BY DEST_COUNTRY_NAME\\\n",
    "          ORDER BY total_count DESC\\\n",
    "          LIMIT 10\\\n",
    "          \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde10b38",
   "metadata": {},
   "source": [
    "3.How many flights originated from the United States?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c6e52a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|    411966|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT SUM(count)\\\n",
    "          FROM summary\\\n",
    "          WHERE ORIGIN_COUNTRY_NAME = 'United States'\\\n",
    "          \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0734f9",
   "metadata": {},
   "source": [
    "4.What are the top 5 origin countries for flights to Japan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "753a1d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|ORIGIN_COUNTRY_NAME|total_count|\n",
      "+-------------------+-----------+\n",
      "|      United States|       1548|\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, SUM(count) as total_count\\\n",
    "          FROM summary\\\n",
    "          WHERE DEST_COUNTRY_NAME = 'Japan'\\\n",
    "          GROUP BY ORIGIN_COUNTRY_NAME\\\n",
    "          ORDER BY total_count DESC\\\n",
    "          LIMIT 5\\\n",
    "          \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6723500",
   "metadata": {},
   "source": [
    "5.What is the total number of flights to the United States?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "d001b070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|    411352|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT SUM(count)\\\n",
    "          FROM summary\\\n",
    "          WHERE DEST_COUNTRY_NAME = 'United States'\\\n",
    "          \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e6c4d2",
   "metadata": {},
   "source": [
    "6.What is the total number of flights from the United States?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "1613a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|    411966|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT SUM(count)\\\n",
    "          FROM summary\\\n",
    "          WHERE ORIGIN_COUNTRY_NAME = 'United States'\\\n",
    "          \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0eb706",
   "metadata": {},
   "source": [
    "7.What are the top 10 origin and destination pairs by count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "5667f8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-----------+\n",
      "|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|total_count|\n",
      "+-------------------+-----------------+-----------+\n",
      "|      United States|    United States|     370002|\n",
      "|             Canada|    United States|       8483|\n",
      "|      United States|           Canada|       8399|\n",
      "|             Mexico|    United States|       7187|\n",
      "|      United States|           Mexico|       7140|\n",
      "|      United States|   United Kingdom|       2025|\n",
      "|     United Kingdom|    United States|       1970|\n",
      "|      United States|            Japan|       1548|\n",
      "|              Japan|    United States|       1496|\n",
      "|      United States|          Germany|       1468|\n",
      "+-------------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
    "          FROM summary\\\n",
    "          GROUP BY ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME\\\n",
    "          ORDER BY total_count DESC\\\n",
    "          LIMIT 10\\\n",
    "\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f17261",
   "metadata": {},
   "source": [
    "8.How many flights originated from each country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "68cb1416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|ORIGIN_COUNTRY_NAME|total_count|\n",
      "+-------------------+-----------+\n",
      "|      United States|     411966|\n",
      "|             Canada|       8483|\n",
      "|             Mexico|       7187|\n",
      "|     United Kingdom|       1970|\n",
      "|              Japan|       1496|\n",
      "| Dominican Republic|       1420|\n",
      "|            Germany|       1336|\n",
      "|        The Bahamas|        986|\n",
      "|             France|        952|\n",
      "|              China|        920|\n",
      "|           Colombia|        867|\n",
      "|        South Korea|        827|\n",
      "|            Jamaica|        712|\n",
      "|        Netherlands|        660|\n",
      "|             Brazil|        619|\n",
      "|         Costa Rica|        608|\n",
      "|        El Salvador|        508|\n",
      "|               Cuba|        478|\n",
      "|             Panama|        465|\n",
      "|              Spain|        442|\n",
      "+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, SUM(count) as total_count\\\n",
    "          FROM summary\\\n",
    "          GROUP BY ORIGIN_COUNTRY_NAME\\\n",
    "          ORDER BY total_count DESC\\\n",
    "\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8dd32",
   "metadata": {},
   "source": [
    "9.How many flights went to each country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "52bc5e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "| DEST_COUNTRY_NAME|total_count|\n",
      "+------------------+-----------+\n",
      "|     United States|     411352|\n",
      "|            Canada|       8399|\n",
      "|            Mexico|       7140|\n",
      "|    United Kingdom|       2025|\n",
      "|             Japan|       1548|\n",
      "|           Germany|       1468|\n",
      "|Dominican Republic|       1353|\n",
      "|       South Korea|       1048|\n",
      "|       The Bahamas|        955|\n",
      "|            France|        935|\n",
      "|          Colombia|        873|\n",
      "|            Brazil|        853|\n",
      "|       Netherlands|        776|\n",
      "|             China|        772|\n",
      "|           Jamaica|        666|\n",
      "|        Costa Rica|        588|\n",
      "|       El Salvador|        561|\n",
      "|            Panama|        510|\n",
      "|              Cuba|        466|\n",
      "|             Spain|        420|\n",
      "+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
    "          FROM summary\\\n",
    "          GROUP BY DEST_COUNTRY_NAME\\\n",
    "          ORDER BY total_count DESC\\\n",
    "\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7f16f",
   "metadata": {},
   "source": [
    "10.What is the total number of flights between the United States and Canada?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "adbd05a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|     16882|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT SUM(count)\\\n",
    "  FROM summary\\\n",
    "  WHERE (ORIGIN_COUNTRY_NAME = 'United States' AND DEST_COUNTRY_NAME = 'Canada') OR (ORIGIN_COUNTRY_NAME = 'Canada' AND DEST_COUNTRY_NAME = 'United States')\\\n",
    "\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0333411b",
   "metadata": {},
   "source": [
    "11.What are the 5 most common origin countries for flights to the United Kingdom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "25893c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|ORIGIN_COUNTRY_NAME|total_count|\n",
      "+-------------------+-----------+\n",
      "|      United States|       2025|\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, SUM(count) as total_count\\\n",
    "  FROM summary\\\n",
    "  WHERE DEST_COUNTRY_NAME = 'United Kingdom'\\\n",
    "  GROUP BY ORIGIN_COUNTRY_NAME\\\n",
    "  ORDER BY total_count DESC\\\n",
    "  LIMIT 5\\\n",
    "\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0809c9c5",
   "metadata": {},
   "source": [
    "12.What are the top 10 destination countries for flights from China?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "68b5b811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|DEST_COUNTRY_NAME|total_count|\n",
      "+-----------------+-----------+\n",
      "|    United States|        920|\n",
      "+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\\\n",
    "  SELECT DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
    "  FROM summary\\\n",
    "  WHERE ORIGIN_COUNTRY_NAME = 'China'\\\n",
    "  GROUP BY DEST_COUNTRY_NAME\\\n",
    "  ORDER BY total_count DESC\\\n",
    "  LIMIT 10\\\n",
    "\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db90db",
   "metadata": {},
   "source": [
    "13. What is the total number of flights between United States and New Zealand?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "14917937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|       185|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT SUM(count)\\\n",
    "  FROM summary\\\n",
    "  WHERE (ORIGIN_COUNTRY_NAME = 'United States' AND DEST_COUNTRY_NAME = 'New Zealand') OR (ORIGIN_COUNTRY_NAME = 'New Zealand' AND DEST_COUNTRY_NAME = 'United States')\\\n",
    "\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be177d",
   "metadata": {},
   "source": [
    "14. What is the total number of flights from India?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "6b07604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|        62|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT SUM(count)\\\n",
    "  FROM summary\\\n",
    "  WHERE ORIGIN_COUNTRY_NAME = 'India'\\\n",
    "\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6767e",
   "metadata": {},
   "source": [
    "15. What is the rank of the destination country with the most flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "ebf85687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 11:13:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:13:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:13:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:13:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:13:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:13:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------------------+-----------+----+\n",
      "| DEST_COUNTRY_NAME|total_count|rank|\n",
      "+------------------+-----------+----+\n",
      "|     United States|     411352|   1|\n",
      "|            Canada|       8399|   2|\n",
      "|            Mexico|       7140|   3|\n",
      "|    United Kingdom|       2025|   4|\n",
      "|             Japan|       1548|   5|\n",
      "|           Germany|       1468|   6|\n",
      "|Dominican Republic|       1353|   7|\n",
      "|       South Korea|       1048|   8|\n",
      "|       The Bahamas|        955|   9|\n",
      "|            France|        935|  10|\n",
      "|          Colombia|        873|  11|\n",
      "|            Brazil|        853|  12|\n",
      "|       Netherlands|        776|  13|\n",
      "|             China|        772|  14|\n",
      "|           Jamaica|        666|  15|\n",
      "|        Costa Rica|        588|  16|\n",
      "|       El Salvador|        561|  17|\n",
      "|            Panama|        510|  18|\n",
      "|              Cuba|        466|  19|\n",
      "|             Spain|        420|  20|\n",
      "+------------------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count,\\\n",
    "  RANK() OVER (ORDER BY SUM(count) DESC) as rank\\\n",
    "  FROM summary\\\n",
    "  GROUP BY DEST_COUNTRY_NAME\\\n",
    "\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc5a6e1",
   "metadata": {},
   "source": [
    "16. What is the rank of the destination country with the most flights from France?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "8f3534cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 11:14:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:14:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:14:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:14:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:14:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:14:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+-----------------+-----------+----+\n",
      "|DEST_COUNTRY_NAME|total_count|rank|\n",
      "+-----------------+-----------+----+\n",
      "|    United States|        952|   1|\n",
      "+-----------------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count,\\\n",
    "  RANK() OVER (ORDER BY SUM(count) DESC) as rank\\\n",
    "  FROM summary\\\n",
    "  WHERE ORIGIN_COUNTRY_NAME = 'France'\\\n",
    "  GROUP BY DEST_COUNTRY_NAME\\\n",
    "\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953520b",
   "metadata": {},
   "source": [
    "17. What is the cumulative sum of flights to each destination country, ordered by the number of flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "c60351aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 11:14:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:14:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:14:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:14:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:14:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:14:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/01/31 11:14:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------------------+-----------+--------------+\n",
      "| DEST_COUNTRY_NAME|total_count|cumulative_sum|\n",
      "+------------------+-----------+--------------+\n",
      "|     United States|     411352|        411352|\n",
      "|            Canada|       8399|        419751|\n",
      "|            Mexico|       7140|        426891|\n",
      "|    United Kingdom|       2025|        428916|\n",
      "|             Japan|       1548|        430464|\n",
      "|           Germany|       1468|        431932|\n",
      "|Dominican Republic|       1353|        433285|\n",
      "|       South Korea|       1048|        434333|\n",
      "|       The Bahamas|        955|        435288|\n",
      "|            France|        935|        436223|\n",
      "|          Colombia|        873|        437096|\n",
      "|            Brazil|        853|        437949|\n",
      "|       Netherlands|        776|        438725|\n",
      "|             China|        772|        439497|\n",
      "|           Jamaica|        666|        440163|\n",
      "|        Costa Rica|        588|        440751|\n",
      "|       El Salvador|        561|        441312|\n",
      "|            Panama|        510|        441822|\n",
      "|              Cuba|        466|        442288|\n",
      "|             Spain|        420|        442708|\n",
      "+------------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count,\\\n",
    "  SUM(SUM(count)) OVER (ORDER BY SUM(count) DESC) as cumulative_sum\\\n",
    "  FROM summary\\\n",
    "  GROUP BY DEST_COUNTRY_NAME\\\n",
    "  ORDER BY total_count DESC\\\n",
    "\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9c65b",
   "metadata": {},
   "source": [
    "# How Spark Runs on a Cluster (chapter 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32deeea6",
   "metadata": {},
   "source": [
    "# SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "8f0b5c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 11:17:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Word Count\")\\\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd0415",
   "metadata": {},
   "source": [
    "# Logical instructions to physical execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "bced07b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 558:======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(7) HashAggregate(keys=[], functions=[sum(id#13405L)])\n",
      "   +- ShuffleQueryStage 4\n",
      "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#8229]\n",
      "         +- *(6) HashAggregate(keys=[], functions=[partial_sum(id#13405L)])\n",
      "            +- *(6) Project [id#13405L]\n",
      "               +- *(6) SortMergeJoin [id#13405L], [id#13399L], Inner\n",
      "                  :- *(4) Sort [id#13405L ASC NULLS FIRST], false, 0\n",
      "                  :  +- ShuffleQueryStage 2\n",
      "                  :     +- Exchange hashpartitioning(id#13405L, 5), ENSURE_REQUIREMENTS, [id=#8108]\n",
      "                  :        +- *(3) Project [(id#13397L * 5) AS id#13405L]\n",
      "                  :           +- ShuffleQueryStage 0\n",
      "                  :              +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [id=#8041]\n",
      "                  :                 +- *(1) Range (2, 10000000, step=2, splits=8)\n",
      "                  +- *(5) Sort [id#13399L ASC NULLS FIRST], false, 0\n",
      "                     +- ShuffleQueryStage 3\n",
      "                        +- Exchange hashpartitioning(id#13399L, 5), ENSURE_REQUIREMENTS, [id=#8053]\n",
      "                           +- ShuffleQueryStage 1\n",
      "                              +- Exchange RoundRobinPartitioning(6), REPARTITION_BY_NUM, [id=#8049]\n",
      "                                 +- *(2) Range (2, 10000000, step=4, splits=8)\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate(keys=[], functions=[sum(id#13405L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#8017]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(id#13405L)])\n",
      "         +- Project [id#13405L]\n",
      "            +- SortMergeJoin [id#13405L], [id#13399L], Inner\n",
      "               :- Sort [id#13405L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(id#13405L, 5), ENSURE_REQUIREMENTS, [id=#8009]\n",
      "               :     +- Project [(id#13397L * 5) AS id#13405L]\n",
      "               :        +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [id=#7999]\n",
      "               :           +- Range (2, 10000000, step=2, splits=8)\n",
      "               +- Sort [id#13399L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#13399L, 5), ENSURE_REQUIREMENTS, [id=#8010]\n",
      "                     +- Exchange RoundRobinPartitioning(6), REPARTITION_BY_NUM, [id=#8002]\n",
      "                        +- Range (2, 10000000, step=4, splits=8)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df1 = spark.range(2, 10000000, 2)\n",
    "df2 = spark.range(2, 10000000, 4)\n",
    "step1 = df1.repartition(5)\n",
    "step12 = df2.repartition(6)\n",
    "step2 = step1.selectExpr(\"id * 5 as id\")\n",
    "step3 = step2.join(step12, [\"id\"])\n",
    "step4 = step3.selectExpr(\"sum(id)\")\n",
    "\n",
    "step4.collect() \n",
    "step4.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc75480",
   "metadata": {},
   "source": [
    "# Spark UI (chapter 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de71be0",
   "metadata": {},
   "source": [
    "When we run Spark in local mode, for example,\n",
    "just navigate to http://localhost:4040 to see the UI when running a Spark Application\n",
    "on local machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cae9c3",
   "metadata": {},
   "source": [
    "• The Jobs tab refers to Spark jobs.\n",
    "\n",
    "• The Stages tab pertains to individual stages (and their relevant tasks).\n",
    "\n",
    "• The Storage tab includes information and the data that is currently cached in our Spark Application.\n",
    "\n",
    "• The Environment tab contains relevant information about the configurations and current settings of the Spark application.\n",
    "\n",
    "• The SQL tab refers to our Structured API queries (including SQL and Data‐Frames).\n",
    "\n",
    "• The Executors tab provides detailed information about each executor running our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "18ae457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(is_glass=False, count=527594),\n",
       " Row(is_glass=None, count=1454),\n",
       " Row(is_glass=True, count=12861)]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read\\\n",
    " .option(\"header\", \"true\")\\\n",
    " .csv(\"data/retail-data/all/online-retail-dataset.csv\")\\\n",
    " .repartition(2)\\\n",
    " .selectExpr(\"instr(Description, 'GLASS') >= 1 as is_glass\")\\\n",
    " .groupBy(\"is_glass\")\\\n",
    " .count()\\\n",
    " .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1f294",
   "metadata": {},
   "source": [
    "# Performance Tuning (chapter 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c643f",
   "metadata": {},
   "source": [
    "# Temporary Data Storage (Caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "e1be5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF1 = spark.read.format(\"csv\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .load(\"data/flight-data/csv/2015-summary.csv\")\n",
    "DF2 = DF1.groupBy(\"DEST_COUNTRY_NAME\").count().collect()\n",
    "DF3 = DF1.groupBy(\"ORIGIN_COUNTRY_NAME\").count().collect()\n",
    "DF4 = DF1.groupBy(\"count\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "ce174685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DF1.cache()\n",
    "DF1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "b59a91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2 = DF1.groupBy(\"DEST_COUNTRY_NAME\").count().collect()\n",
    "DF3 = DF1.groupBy(\"ORIGIN_COUNTRY_NAME\").count().collect()\n",
    "DF4 = DF1.groupBy(\"count\").count().collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
